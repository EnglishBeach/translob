{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%mkdir models\n",
    "%mkdir tools\n",
    "%mkdir saved_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing models/__init__.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile models/__init__.py\n",
    "__all__ = [\"I will get rewritten\"]\n",
    "# Don't modify the line above, or this line!\n",
    "import automodinit as  _automodinit\n",
    "_automodinit.automodinit(__name__, __file__, globals())\n",
    "del _automodinit\n",
    "# Anything else you want can go after here, it won't get modified."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile models/m_base.py\n",
    "\"\"\"\n",
    "Realisation transformer from article\n",
    "\"\"\"\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.utils import get_custom_objects as _get_custom_objects\n",
    "from tensorflow.keras import backend as _K\n",
    "\n",
    "from typing import Union as _Union\n",
    "from typing import Callable as _Callable\n",
    "\n",
    "import keras\n",
    "\n",
    "keras\n",
    "tf.keras\n",
    "\n",
    "\n",
    "# Input\n",
    "def input_block(seq_len):\n",
    "    inputs = tf.keras.Input(shape=(seq_len, 40))\n",
    "    return inputs\n",
    "\n",
    "\n",
    "# CN\n",
    "def cnn_block(\n",
    "    input_layer,\n",
    "    filters,\n",
    "    dilation_steps,\n",
    "):\n",
    "    x = input_layer\n",
    "    x = tf.keras.layers.Conv1D(\n",
    "        14,\n",
    "        kernel_size=2,\n",
    "        strides=1,\n",
    "        activation='relu',\n",
    "        padding='causal',\n",
    "    )(x)\n",
    "\n",
    "    dilation_steps = [\n",
    "        2**dilation\n",
    "        for dilation in range(1,dilation_steps + 1)\n",
    "    ] # yapf: disable\n",
    "    for dilation in dilation_steps:\n",
    "        layer = tf.keras.layers.Conv1D(\n",
    "            filters=filters,\n",
    "            kernel_size=2,\n",
    "            dilation_rate=dilation,\n",
    "            activation='relu',\n",
    "            padding='causal',\n",
    "        )\n",
    "        x = layer(x)\n",
    "    return x\n",
    "\n",
    "\n",
    "# Normalisation\n",
    "def norm_block(input_layer):\n",
    "\n",
    "    norm = tf.keras.layers.LayerNormalization()(input_layer)\n",
    "    return norm\n",
    "\n",
    "\n",
    "# Positional encoding\n",
    "class PositionalEncoding(tf.keras.layers.Layer):\n",
    "\n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "\n",
    "    def call(self, x, *args, **kwargs):\n",
    "        steps, d_model = x.get_shape()[-2:]\n",
    "        ps = np.zeros([steps, 1], dtype=_K.floatx())\n",
    "        for step in range(steps):\n",
    "            ps[step, :] = [(2 / (steps - 1)) * step - 1]\n",
    "\n",
    "        ps_expand = _K.expand_dims(_K.constant(ps), axis=0)\n",
    "        ps_tiled = _K.tile(ps_expand, [_K.shape(x)[0], 1, 1])\n",
    "\n",
    "        x = _K.concatenate([x, ps_tiled], axis=-1)\n",
    "        return x\n",
    "\n",
    "\n",
    "_get_custom_objects().update({\n",
    "    'PositionalEncoding': PositionalEncoding,\n",
    "})\n",
    "\n",
    "\n",
    "def positional_encoder_block(input_layer):\n",
    "    pos = PositionalEncoding()(input_layer)\n",
    "    return pos\n",
    "\n",
    "\n",
    "# Transformer\n",
    "class MultiHeadSelfAttention(tf.keras.layers.Layer):\n",
    "    \"\"\"\n",
    "    Base class for Multi-head Self-Attention layers.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, num_heads: int, use_masking: bool, **kwargs):\n",
    "        \"\"\"\n",
    "        :param num_heads: number of attention heads\n",
    "        :param use_masking: when True, forbids the attention to see the further\n",
    "          elements in the sequence.\n",
    "        :param kwargs: any extra arguments typical for a tf.Keras layer,\n",
    "          such as name, etc.\n",
    "        \"\"\"\n",
    "        self.num_heads = num_heads\n",
    "        self.use_masking = use_masking\n",
    "        self.qkv_weights = None\n",
    "        super().__init__(**kwargs)\n",
    "\n",
    "    def get_config(self):\n",
    "        config = super().get_config()\n",
    "        config['num_heads'] = self.num_heads\n",
    "        config['use_masking'] = self.use_masking\n",
    "        return config\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        # if not isinstance(input_shape, TensorShape):\n",
    "        #     raise ValueError('Invalid input')\n",
    "        d_model = input_shape[-1]\n",
    "\n",
    "        self.validate_model_dimensionality(d_model)\n",
    "        self.qkv_weights = self.add_weight(\n",
    "            name='qkv_weights',\n",
    "            shape=(d_model, d_model * 3),  # * 3 for q, k and v\n",
    "            initializer='glorot_uniform',\n",
    "            trainable=True)\n",
    "\n",
    "        return super().build(input_shape)\n",
    "\n",
    "    def call(self, inputs, **kwargs):\n",
    "        # if not K.is_tf.keras_tensor(inputs):\n",
    "        #     raise ValueError(\n",
    "        #         'The layer can be called only with one tensor as an argument')\n",
    "        _, seq_len, d_model = _K.int_shape(inputs)\n",
    "\n",
    "        # Perform affine transformations to get the Queries, the Keys and the Values.\n",
    "        qkv = _K.dot(inputs, self.qkv_weights)  # (-1,seq_len,d_model*3)\n",
    "        qkv = _K.reshape(qkv, [-1, d_model * 3])\n",
    "\n",
    "        # splitting the keys, the values and the queries.\n",
    "        pre_q, pre_k, pre_v = [\n",
    "            _K.reshape(\n",
    "                qkv[:, i * d_model:(i + 1) * d_model],\n",
    "                (-1, seq_len, self.num_heads, d_model // self.num_heads))\n",
    "            for i in range(3)\n",
    "        ]\n",
    "\n",
    "        attention_out = self.attention(\n",
    "            pre_q,\n",
    "            pre_v,\n",
    "            pre_k,\n",
    "            seq_len,\n",
    "            d_model,\n",
    "            training=kwargs.get('training'),\n",
    "        )\n",
    "        # of shape (-1, seq_len, d_model)\n",
    "        return attention_out\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        shape_a, seq_len, d_model = input_shape\n",
    "        return shape_a, seq_len, d_model\n",
    "\n",
    "    def validate_model_dimensionality(self, d_model: int):\n",
    "        if d_model % self.num_heads != 0:\n",
    "            raise ValueError(\n",
    "                f'The size of the last dimension of the input '\n",
    "                f'({d_model}) must be evenly divisible by the number'\n",
    "                f'of the attention heads {self.num_heads}')\n",
    "\n",
    "    def attention(\n",
    "        self,\n",
    "        pre_q,\n",
    "        pre_v,\n",
    "        pre_k,\n",
    "        seq_len: int,\n",
    "        d_model: int,\n",
    "        training=None,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Calculates the output of the attention once the affine transformations\n",
    "        of the inputs are done. Here's the shapes of the arguments:\n",
    "        :param pre_q: (batch_size, q_seq_len, num_heads, d_model // num_heads)\n",
    "        :param pre_v: (batch_size, v_seq_len, num_heads, d_model // num_heads)\n",
    "        :param pre_k: (batch_size, k_seq_len, num_heads, d_model // num_heads)\n",
    "        :param seq_len: the length of the output sequence\n",
    "        :param d_model: dimensionality of the model (by the paper)\n",
    "        :param training: Passed by tf.Keras. Should not be defined manually.\n",
    "          Optional scalar tensor indicating if we're in training\n",
    "          or inference phase.\n",
    "        \"\"\"\n",
    "        d_submodel = d_model // self.num_heads\n",
    "\n",
    "        # shaping Q and V into (batch_size, num_heads, seq_len, d_model//heads)\n",
    "        q = _K.permute_dimensions(pre_q, [0, 2, 1, 3])\n",
    "        v = _K.permute_dimensions(pre_v, [0, 2, 1, 3])\n",
    "        k = _K.permute_dimensions(pre_k, [0, 2, 3, 1])\n",
    "\n",
    "        q = _K.reshape(q, (-1, seq_len, d_submodel))\n",
    "        k = _K.reshape(k, (-1, seq_len, d_submodel))\n",
    "        v = _K.reshape(v, (-1, seq_len, d_submodel))\n",
    "        qk = tf.einsum('aib,ajb->aij', q, k)\n",
    "        sqrt_d = _K.constant(np.sqrt(d_model // self.num_heads),\n",
    "                             dtype=_K.floatx())\n",
    "        a = qk / sqrt_d\n",
    "        a = self.mask_attention(a)\n",
    "        a = _K.softmax(a)\n",
    "        attention_heads = tf.einsum('aij,ajb->aib', a, v)\n",
    "        attention_heads = _K.reshape(attention_heads,\n",
    "                                     (-1, self.num_heads, seq_len, d_submodel))\n",
    "        attention_heads = _K.permute_dimensions(attention_heads, [0, 2, 1, 3])\n",
    "        attention_heads = _K.reshape(attention_heads, (-1, seq_len, d_model))\n",
    "\n",
    "        return attention_heads\n",
    "\n",
    "    def mask_attention(self, dot_product):\n",
    "        \"\"\"\n",
    "        Makes sure that (when enabled) each position\n",
    "        (of a decoder's self-attention) cannot attend to subsequent positions.\n",
    "        :param dot_product: scaled dot-product of Q and K after reshaping them\n",
    "        to 3D tensors (batch * num_heads, rows, cols)\n",
    "        \"\"\"\n",
    "        if not self.use_masking:\n",
    "            return dot_product\n",
    "        last_dims = _K.int_shape(dot_product)[-2:]\n",
    "        low_triangle_ones = (\n",
    "            np.tril(np.ones(last_dims))\n",
    "            # to ensure proper broadcasting\n",
    "            .reshape((1, ) + last_dims))\n",
    "        inverse_low_triangle = 1 - low_triangle_ones\n",
    "        close_to_negative_inf = -1e9\n",
    "        result = (\n",
    "            _K.constant(low_triangle_ones, dtype=_K.floatx()) * dot_product +\n",
    "            _K.constant(close_to_negative_inf * inverse_low_triangle))\n",
    "        return result\n",
    "\n",
    "\n",
    "_get_custom_objects().update({\n",
    "    'MultiHeadSelfAttention': MultiHeadSelfAttention,\n",
    "})\n",
    "\n",
    "\n",
    "class CustomNormalization(tf.keras.layers.Layer):\n",
    "    \"\"\"\n",
    "    Implementation of Layer Normalization (https://arxiv.org/abs/1607.06450).\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, axis=-1, **kwargs):\n",
    "        self.axis = axis\n",
    "        super().__init__(**kwargs)\n",
    "\n",
    "    def get_config(self):\n",
    "        config = super().get_config()\n",
    "        config['axis'] = self.axis\n",
    "        return config\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        dim = input_shape[-1]\n",
    "        self.gain = self.add_weight(\n",
    "            name='gain',\n",
    "            shape=(dim, ),\n",
    "            initializer='ones',\n",
    "            trainable=True,\n",
    "        )\n",
    "        self.bias = self.add_weight(\n",
    "            name='bias',\n",
    "            shape=(dim, ),\n",
    "            initializer='zeros',\n",
    "            trainable=True,\n",
    "        )\n",
    "        return super().build(input_shape)\n",
    "\n",
    "    def call(self, inputs, **kwargs):\n",
    "        mean = _K.mean(\n",
    "            inputs,\n",
    "            axis=self.axis,\n",
    "            keepdims=True,\n",
    "        )\n",
    "        variance = _K.mean(\n",
    "            _K.square(inputs - mean),\n",
    "            axis=self.axis,\n",
    "            keepdims=True,\n",
    "        )\n",
    "        epsilon = _K.constant(\n",
    "            1e-5,\n",
    "            dtype=_K.floatx(),\n",
    "        )\n",
    "        normalized_inputs = (inputs - mean) / _K.sqrt(variance + epsilon)\n",
    "        result = self.gain * normalized_inputs + self.bias\n",
    "        return result\n",
    "\n",
    "\n",
    "_get_custom_objects().update({\n",
    "    'CustomNormalization': CustomNormalization,\n",
    "})\n",
    "\n",
    "\n",
    "class TransformerTransition(tf.keras.layers.Layer):\n",
    "    \"\"\"\n",
    "    Transformer transition function. The same function is used both\n",
    "    in classical in Universal Transformers.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        activation: _Union[str, _Callable],\n",
    "        size_multiplier: int = 4,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        :param activation: activation function. Must be a string or a callable.\n",
    "        :param size_multiplier: How big the hidden dimension should be.\n",
    "          Most of the implementation use transition functions having 4 times\n",
    "          more hidden units than the model itself.\n",
    "        :param kwargs: tf.Keras-specific layer arguments.\n",
    "        \"\"\"\n",
    "        self.activation = tf.keras.activations.get(activation)\n",
    "        self.size_multiplier = size_multiplier\n",
    "        super().__init__(**kwargs)\n",
    "\n",
    "    def get_config(self):\n",
    "        config = super().get_config()\n",
    "        config['activation'] = tf.keras.activations.serialize(self.activation)\n",
    "        config['size_multiplier'] = self.size_multiplier\n",
    "        return config\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        d_model = input_shape[-1]\n",
    "        self.weights1 = self.add_weight(\n",
    "            name='weights1',\n",
    "            shape=(d_model, self.size_multiplier * d_model),\n",
    "            initializer='glorot_uniform',\n",
    "            trainable=True,\n",
    "        )\n",
    "        self.biases1 = self.add_weight(\n",
    "            name='biases1',\n",
    "            shape=(self.size_multiplier * d_model),\n",
    "            initializer='zeros',\n",
    "            trainable=True,\n",
    "        )\n",
    "        self.weights2 = self.add_weight(\n",
    "            name='weights2',\n",
    "            shape=(self.size_multiplier * d_model, d_model),\n",
    "            initializer='glorot_uniform',\n",
    "            trainable=True,\n",
    "        )\n",
    "        self.biases2 = self.add_weight(\n",
    "            name='biases2',\n",
    "            shape=(d_model, ),\n",
    "            initializer='zeros',\n",
    "            trainable=True,\n",
    "        )\n",
    "        return super().build(input_shape)\n",
    "\n",
    "    def call(self, inputs, **kwargs):\n",
    "        input_shape = _K.int_shape(inputs)\n",
    "        d_model = input_shape[-1]\n",
    "\n",
    "        K_dot = _K.dot(_K.reshape(inputs, (-1, d_model)), self.weights1)\n",
    "        step1 = self.activation(\n",
    "            _K.bias_add(K_dot, self.biases1, data_format='channels_last'))\n",
    "\n",
    "        K_dot = _K.dot(step1, self.weights2)\n",
    "        step2 = _K.bias_add(K_dot, self.biases2, data_format='channels_last')\n",
    "        result = _K.reshape(step2, (-1, ) + input_shape[-2:])\n",
    "        return result\n",
    "\n",
    "\n",
    "_get_custom_objects().update({\n",
    "    'TransformerTransition': TransformerTransition,\n",
    "})\n",
    "\n",
    "\n",
    "class TransformerLayer(tf.keras.layers.Layer):\n",
    "    \"\"\"\n",
    "    A pseudo-layer combining together all nuts and bolts to assemble\n",
    "    a complete section of both the Transformer and the Universal Transformer\n",
    "    models, following description from the \"Universal Transformers\" paper.\n",
    "    Each such block is, essentially:\n",
    "    - Multi-head self-attention (masked or unmasked)\n",
    "    - Residual connection,\n",
    "    - Layer normalization\n",
    "    - Transition function\n",
    "    - Residual connection\n",
    "    - Layer normalization\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        # name: str,\n",
    "        num_heads: int,\n",
    "        use_masking: bool = True,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        self.num_heads = num_heads\n",
    "        self.use_masking = use_masking\n",
    "        self.attention_layer = MultiHeadSelfAttention(\n",
    "            num_heads,\n",
    "            use_masking=use_masking,\n",
    "            # name=f'{name}_self_attention',\n",
    "        )\n",
    "        self.norm1_layer = CustomNormalization()\n",
    "        self.norm2_layer = CustomNormalization()\n",
    "        self.transition_layer = TransformerTransition(activation='relu', )\n",
    "        self.addition_layer = tf.keras.layers.Add()\n",
    "        super().__init__(**kwargs)\n",
    "\n",
    "    def get_config(self):\n",
    "        config = super().get_config()\n",
    "        config.update({\n",
    "            \"num_heads\": self.num_heads,\n",
    "            \"use_masking\": self.use_masking,\n",
    "        })\n",
    "        return config\n",
    "\n",
    "    def call(self, x, **kwargs):\n",
    "        #PostLN: X -> attention -> +X -> norm1 -> transition -> +norm1 -> norm2\n",
    "        attention = self.attention_layer(x)\n",
    "        residual_1 = (self.addition_layer([x, attention]))\n",
    "        norm_1 = self.norm1_layer(residual_1)\n",
    "\n",
    "        transition = self.transition_layer(norm_1)\n",
    "        residual_2 = (self.addition_layer([norm_1, transition]))\n",
    "        norm_2 = self.norm2_layer(residual_2)\n",
    "\n",
    "        return norm_2\n",
    "\n",
    "\n",
    "_get_custom_objects().update({\n",
    "    'TransformerLayer': TransformerLayer,\n",
    "})\n",
    "\n",
    "\n",
    "def transformer_block(\n",
    "    input_layer,\n",
    "    share_weights,\n",
    "    blocks,\n",
    "    heads,\n",
    "):\n",
    "    x = input_layer\n",
    "    tb = TransformerLayer(\n",
    "        num_heads=heads,\n",
    "        use_masking=True,\n",
    "    )\n",
    "    for block in range(blocks):\n",
    "        if share_weights:\n",
    "            x = tb(x)\n",
    "        else:\n",
    "            x = TransformerLayer(\n",
    "                num_heads=heads,\n",
    "                use_masking=True,\n",
    "            )(x)\n",
    "\n",
    "    return x\n",
    "\n",
    "\n",
    "# FFN\n",
    "def ffn_block(\n",
    "    input_layer,\n",
    "    dropout_rate,\n",
    "    activation,\n",
    "    units,\n",
    "    kernel_regularizer,\n",
    "    kernel_initializer,\n",
    "    out_activation,\n",
    "):\n",
    "    input_layer = tf.keras.layers.Flatten()(input_layer)\n",
    "\n",
    "    input_layer = tf.keras.layers.Dense(\n",
    "        units=units,\n",
    "        activation=activation,\n",
    "        kernel_regularizer=kernel_regularizer,\n",
    "        kernel_initializer=kernel_initializer,\n",
    "    )(input_layer)\n",
    "\n",
    "    input_layer = tf.keras.layers.Dropout(dropout_rate)(input_layer)\n",
    "    out = tf.keras.layers.Dense(\n",
    "        units=3,\n",
    "        activation=out_activation,\n",
    "    )(input_layer)\n",
    "    return out\n",
    "\n",
    "\n",
    "# parametrs\n",
    "PARAMETRS = {\n",
    "    'seq_len': 100,\n",
    "    'convolutional': dict(\n",
    "        filters=14,\n",
    "        dilation_steps=4,\n",
    "    ),\n",
    "    'transformer': dict(\n",
    "        heads=3,\n",
    "        blocks=2,\n",
    "        share_weights=False,\n",
    "    ),\n",
    "    'feed_forward': dict(\n",
    "        units = 64,\n",
    "        dropout_rate=0.1,\n",
    "        activation=tf.keras.activations.relu,\n",
    "        kernel_regularizer=tf.keras.regularizers.L2(),\n",
    "        kernel_initializer='glorot_uniform',\n",
    "        out_activation='softmax',\n",
    "    ),\n",
    "    'optimizer':\n",
    "    tf.keras.optimizers.legacy.Adam(\n",
    "        learning_rate=0.0001,\n",
    "        beta_1=0.9,\n",
    "        beta_2=0.999,\n",
    "    ),\n",
    "} #yapf:disable\n",
    "\n",
    "\n",
    "# build\n",
    "class blocks:\n",
    "    input_block = input_block\n",
    "    cnn_block = cnn_block\n",
    "    norm_block = norm_block\n",
    "    positional_encoder_block = positional_encoder_block\n",
    "    transformer_block = transformer_block\n",
    "    ffn_block = ffn_block\n",
    "\n",
    "    def build_model(seq_len, convolutional, transformer, feed_forward,\n",
    "                    optimizer):\n",
    "        # Model\n",
    "        inputs = blocks.input_block(seq_len)\n",
    "        x = inputs\n",
    "        x = blocks.cnn_block(input_layer=x, **convolutional)\n",
    "        x = blocks.norm_block(input_layer=x)\n",
    "        x = blocks.positional_encoder_block(input_layer=x)\n",
    "        x = blocks.transformer_block(input_layer=x, **transformer)\n",
    "        x = blocks.ffn_block(input_layer=x, **feed_forward)\n",
    "\n",
    "        model = tf.keras.Model(inputs=inputs, outputs=x)\n",
    "\n",
    "        # Compile\n",
    "        model.compile(\n",
    "            optimizer,\n",
    "            loss=tf.keras.losses.SparseCategoricalCrossentropy(),\n",
    "            metrics=[\n",
    "                tf.keras.metrics.SparseCategoricalAccuracy(name='sp_acc'),\n",
    "                tf.keras.metrics.CategoricalAccuracy(name='acc'),\n",
    "            ],\n",
    "        )\n",
    "        return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile tools/express.py\n",
    "import os\n",
    "import numpy as _np\n",
    "import tensorflow as tf\n",
    "\n",
    "from .utils import inspect_data, inspect_dataset, build_dataset\n",
    "\n",
    "# download FI2010 dataset from\n",
    "# https://etsin.fairdata.fi/dataset/73eb48d7-4dbc-4a10-a52a-da745b47a649\n",
    "\n",
    "\n",
    "# Paths\n",
    "def check_using_jupyter():\n",
    "    try:\n",
    "        get_ipython().__class__.__name__\n",
    "        using_jupyter = True\n",
    "    except NameError:\n",
    "        using_jupyter = False\n",
    "\n",
    "    global prefix, save_path, dataset_path, callback_path\n",
    "    prefix = '..' if using_jupyter else '.'\n",
    "    save_path = prefix + r'/LOB/saved_data'\n",
    "    dataset_path = prefix + r'/dataset/BenchmarkDatasets/NoAuction/1.NoAuction_Zscore/NoAuction_Zscore'\n",
    "    callback_path = prefix + f'/Temp/callbacks'\n",
    "\n",
    "\n",
    "check_using_jupyter()\n",
    "\n",
    "\n",
    "# Save\n",
    "def save_data(x, y, name):\n",
    "    \"\"\"\n",
    "    kinds = 'test', 'train', 'val'\n",
    "    \"\"\"\n",
    "    with open(f'{save_path}/x_{name}.npy', 'wb') as file:\n",
    "        _np.save(file, x)\n",
    "    with open(f'{save_path}/y_{name}.npy', 'wb') as file:\n",
    "        _np.save(file, y)\n",
    "\n",
    "\n",
    "# Load data\n",
    "def _gen_data(data, horizon):\n",
    "    x = data[:40, :].T  # 40 == 10 price + volume asks + 10 price + volume bids\n",
    "    y = data[-5 + horizon, :].T  # 5\n",
    "    return [x[:-1], (y[1:] - 1).astype(_np.int32)]  # shift y by 1\n",
    "\n",
    "\n",
    "def load_datas(horizon):\n",
    "    dec_data = _np.loadtxt(\n",
    "        f'{dataset_path}_Training/Train_Dst_NoAuction_ZScore_CF_7.txt')\n",
    "\n",
    "    dec_train = dec_data[:, :int(_np.floor(dec_data.shape[1] * 0.8))]\n",
    "    dec_val = dec_data[:, int(_np.floor(dec_data.shape[1] * 0.8)):]\n",
    "\n",
    "    dec_test1 = _np.loadtxt(\n",
    "        f'{dataset_path}_Testing/Test_Dst_NoAuction_ZScore_CF_7.txt')\n",
    "\n",
    "    dec_test2 = _np.loadtxt(\n",
    "        f'{dataset_path}_Testing/Test_Dst_NoAuction_ZScore_CF_8.txt')\n",
    "\n",
    "    dec_test3 = _np.loadtxt(\n",
    "        f'{dataset_path}_Testing/Test_Dst_NoAuction_ZScore_CF_9.txt')\n",
    "\n",
    "    dec_test = _np.hstack((dec_test1, dec_test2, dec_test3))\n",
    "\n",
    "    datas = {\n",
    "        'train': _gen_data(dec_train, horizon),\n",
    "        'val': _gen_data(dec_val, horizon),\n",
    "        'test': _gen_data(dec_test, horizon),\n",
    "    }\n",
    "    return datas\n",
    "\n",
    "\n",
    "def load_saved_datas(part=1):\n",
    "    \"\"\"\n",
    "    kinds = 'test', 'train', 'val'\n",
    "    \"\"\"\n",
    "    datas = {}\n",
    "    for kind in ['train', 'val', 'test']:\n",
    "        try:\n",
    "            with open(f'{save_path}/x_{kind}.npy', 'rb') as file:\n",
    "                x = _np.load(file)\n",
    "            with open(f'{save_path}/y_{kind}.npy', 'rb') as file:\n",
    "                y = _np.load(file)\n",
    "            data_len = int(len(x) * part)\n",
    "            x = x[:data_len]\n",
    "            y = y[:data_len]\n",
    "\n",
    "        except FileNotFoundError:\n",
    "            x, y = None, None\n",
    "\n",
    "        datas.update({kind: [x, y]})\n",
    "    return datas\n",
    "\n",
    "\n",
    "def inspect_datas(datas: dict):\n",
    "    print('    Datas:')\n",
    "    for name in datas:\n",
    "        data = datas[name]\n",
    "        inspect_data(data, name)\n",
    "\n",
    "\n",
    "def build_datasets(datas: dict, batch_size, seq_len):\n",
    "    datasets = {}\n",
    "    for kind in datas:\n",
    "        data = datas.get(kind, None)\n",
    "        ds = None\n",
    "        if data is not None:\n",
    "            ds = build_dataset(\n",
    "                x=data[0],\n",
    "                y=data[1],\n",
    "                batch_size=batch_size,\n",
    "                seq_len=seq_len,\n",
    "            )\n",
    "        datasets.update({kind: ds})\n",
    "\n",
    "    return datasets\n",
    "\n",
    "\n",
    "def inspect_datasets(datasets: dict):\n",
    "    print('    Datasets:')\n",
    "    for name in datasets:\n",
    "        ds = datasets[name]\n",
    "        inspect_dataset(ds, name)\n",
    "\n",
    "\n",
    "def restore_model(input_name):\n",
    "    restore_path = f'{callback_path}/{input_name}/checkpoints'\n",
    "    checkpoint_list = sorted(os.listdir(restore_path))\n",
    "\n",
    "    model = tf.keras.models.load_model(f'{restore_path}/{checkpoint_list[-1]}')\n",
    "\n",
    "    print(f'Model {checkpoint_list[-1]} loaded')\n",
    "    restored_epoch = checkpoint_list[-1].split('.')[0]\n",
    "    restored_name = input_name.split('(')[0]\n",
    "\n",
    "    return model, f'restore_{restored_epoch}_{restored_name}'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile tools/express.py\n",
    "import numpy as _np\n",
    "import pandas as _pd\n",
    "\n",
    "from tensorflow.keras.utils import timeseries_dataset_from_array as _timeseries_dataset_from_array\n",
    "\n",
    "\n",
    "class DataClass:\n",
    "    \"\"\"\n",
    "    make only lover case parametrs and not start with _\n",
    "    All this methods (exept __call__) only for beauty representation :)\n",
    "    \"\"\"\n",
    "\n",
    "    @staticmethod\n",
    "    def __not_data(field=None, get=False, not_data_fields: set = set()):\n",
    "        if not get:\n",
    "            not_data_fields.add(field.__name__)\n",
    "            return field\n",
    "        else:\n",
    "            return not_data_fields\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        target_dict: dict = None,\n",
    "        name: str = '',\n",
    "    ):\n",
    "        for field_name in self.__get_all_fields():\n",
    "            field = getattr(self, field_name)\n",
    "            if type(self.__init__) == type(field):\n",
    "                # TODO: add signature\n",
    "                field_result = field.__func__\n",
    "                setattr(self, field_name, field_result)\n",
    "\n",
    "    def __new__(\n",
    "        cls,\n",
    "        target_dict: dict = None,\n",
    "        name: str = '',\n",
    "    ):\n",
    "        \"\"\"\n",
    "        build from nested dict\n",
    "        \"\"\"\n",
    "        if target_dict is not None:\n",
    "            result = DataClass()\n",
    "            return result.__rec_build(name, target_dict)\n",
    "        return super().__new__(cls)\n",
    "\n",
    "    def __rec_build(self, field_name: str, field):\n",
    "        if not isinstance(field, dict):\n",
    "            self.__setattr__(field_name, field)\n",
    "            return None\n",
    "\n",
    "        result = DataClass()\n",
    "        self.__setattr__(field_name, result)\n",
    "\n",
    "        for inner_field_name, inner_field in field.items():\n",
    "            inner_result = result.__rec_build(\n",
    "                inner_field_name,\n",
    "                inner_field,\n",
    "            )\n",
    "            if inner_result is not None:\n",
    "                self.__setattr__(field_name, inner_result)\n",
    "        return result\n",
    "\n",
    "    def __call__(self, **kwargs: dict):\n",
    "        \"\"\"\n",
    "        Set up parametrs\n",
    "        \"\"\"\n",
    "        for key, value in kwargs.items():\n",
    "            self.__setattr__(key, value)\n",
    "\n",
    "    def __get_all_fields(self):\n",
    "        # Add except fields\n",
    "\n",
    "        options = list(\n",
    "            filter(\n",
    "                lambda x:\n",
    "                (x[0] != '_') and (x not in self.__not_data(get=True)),\n",
    "                self.__dir__(),\n",
    "            ))\n",
    "        return options\n",
    "\n",
    "    def __repr__(self) -> str:\n",
    "        \"\"\"\n",
    "        Representation of options\n",
    "        \"\"\"\n",
    "        return self.__rec_print()[4:]\n",
    "\n",
    "    def _rec_print_depr(self, self_margin: str = ''):\n",
    "        if not isinstance(self, DataClass):\n",
    "            return f'{self}'\n",
    "\n",
    "        result = self_margin\n",
    "        for field_name in self.__get_all_fields():\n",
    "            inner_result = DataClass._rec_print_depr(\n",
    "                self.__getattribute__(field_name),\n",
    "                self_margin + ' ' * 4,\n",
    "            )\n",
    "            result += f'\\n{self_margin}{field_name}: {inner_result}'\n",
    "\n",
    "        if self_margin == '':\n",
    "            return result[1:]\n",
    "        else:\n",
    "            return result\n",
    "\n",
    "    def __rec_print(\n",
    "        self,\n",
    "        self_name: str = '',\n",
    "        self_header: str = '',\n",
    "        last=True,\n",
    "    ):\n",
    "        end = \"└─ \"\n",
    "        pipe = \"│  \"\n",
    "        tee = \"├─ \"\n",
    "        blank = \"   \"\n",
    "        result = f'{self_header}{end if last else tee}{self_name}\\n'\n",
    "\n",
    "        if not isinstance(self, DataClass):\n",
    "            if '<' in repr(self):\n",
    "                self = repr(self).split('at')[0].replace('<', '').strip()\n",
    "\n",
    "            return f'{self_header}{end if last else tee}{self_name}: {self}\\n'\n",
    "\n",
    "        fields = self.__get_all_fields()\n",
    "        for field_name in fields:\n",
    "            inner_result = DataClass.__rec_print(\n",
    "                self.__getattribute__(field_name),\n",
    "                self_name=field_name,\n",
    "                self_header=f'{self_header}{blank if last else pipe}',\n",
    "                last=field_name == fields[-1])\n",
    "\n",
    "            result += inner_result[6:]\n",
    "\n",
    "        return result\n",
    "\n",
    "    @property\n",
    "    @__not_data\n",
    "    def Data_nested(self):\n",
    "        \"\"\"\n",
    "        Containing options dict\n",
    "        \"\"\"\n",
    "        return self.__rec_nested()\n",
    "\n",
    "    def __rec_nested(self, self_name=None):\n",
    "        if not isinstance(self, DataClass):\n",
    "            return {self_name: self}\n",
    "\n",
    "        result = {}\n",
    "        for field_name in self.__get_all_fields():\n",
    "            inner_result = DataClass.__rec_nested(\n",
    "                self.__getattribute__(field_name),\n",
    "                field_name,\n",
    "            )\n",
    "            result.update(inner_result)\n",
    "\n",
    "        if self_name is None:\n",
    "            return result\n",
    "        else:\n",
    "            return {self_name: result}\n",
    "\n",
    "    @property\n",
    "    @__not_data\n",
    "    def Data_expanded(self):\n",
    "        return {\n",
    "            compound_key.strip()[2:]: value\n",
    "            for value, compound_key in self.__rec_expanded()\n",
    "        }\n",
    "\n",
    "    def __rec_expanded(self, composite_key=''):\n",
    "        if not isinstance(self, DataClass):\n",
    "            yield (self, composite_key)\n",
    "        else:\n",
    "            for field_name in self.__get_all_fields():\n",
    "                for inner_result in DataClass.__rec_expanded(\n",
    "                        self.__getattribute__(field_name),\n",
    "                        str(composite_key) + '__' + str(field_name),\n",
    "                ):\n",
    "                    yield inner_result\n",
    "\n",
    "    def __getitem__(self, value):\n",
    "        if isinstance(value, list | tuple):\n",
    "            result = {}\n",
    "            for i in value:\n",
    "                result.update({i: getattr(self, i, None)})\n",
    "            return DataClass(result)\n",
    "        result = getattr(self, value, None)\n",
    "        if isinstance(result, DataClass):\n",
    "            return DataClass(result.Data_nested)\n",
    "        else:\n",
    "            return result\n",
    "\n",
    "\n",
    "def build_dataset(\n",
    "    x: _np.ndarray,\n",
    "    y: _np.ndarray,\n",
    "    seq_len,\n",
    "    batch_size=128,\n",
    "    **timeseries_kwargs,\n",
    "):\n",
    "\n",
    "    def set_shape(value_x, value_y):\n",
    "        value_x.set_shape((None, seq_len, x.shape[-1]))\n",
    "        return value_x, value_y\n",
    "\n",
    "    ds = _timeseries_dataset_from_array(\n",
    "        data=x,\n",
    "        targets=y,\n",
    "        batch_size=batch_size,\n",
    "        sequence_length=seq_len,\n",
    "        **timeseries_kwargs,\n",
    "    )\n",
    "\n",
    "    return ds.map(set_shape)\n",
    "\n",
    "\n",
    "def inspect_data(data, name='data'):\n",
    "    if data is not None:\n",
    "        x = data[0]\n",
    "        y = data[1]\n",
    "        print(f'{name: <10}: x= {str(x.shape): <15} | y= {str(y.shape): <15}')\n",
    "    else:\n",
    "        print(f'{name <10}: None')\n",
    "\n",
    "\n",
    "def inspect_dataset(ds: _pd.DataFrame, name='dataset'):\n",
    "    if ds is not None:\n",
    "        print(f'{name: <10}: {[len(ds)]+ list(ds.element_spec[0].shape)[1:]}')\n",
    "    else:\n",
    "        print(f'{name <10}: None')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.0 ('zGPU_env': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "15c3c8882de147eda8616aa412d6cb2a921cdd19c8088b1f5bdfa95af6065bbb"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
