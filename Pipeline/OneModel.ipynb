{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Use: jupyter\n",
            "Using TensorFlow backend\n",
            "Dataset  : ../dataset/saved_data\n",
            "Callbacks: ../Temp/callbacks\n"
          ]
        }
      ],
      "source": [
        "## For platforms\n",
        "import os\n",
        "\n",
        "\n",
        "def get_platform():\n",
        "    platform = ''\n",
        "\n",
        "    # Windows\n",
        "    if os.name == 'nt':\n",
        "        try:\n",
        "            get_ipython().__class__.__name__\n",
        "            platform = 'jupyter'\n",
        "        except NameError:\n",
        "            platform = 'python'\n",
        "\n",
        "    elif os.name == 'posix':\n",
        "        # Kaggle\n",
        "        if 'KAGGLE_DATA_PROXY_TOKEN' in os.environ.keys():\n",
        "            platform = 'kaggle'\n",
        "\n",
        "    # Google Colab\n",
        "        else:\n",
        "            try:\n",
        "                from google.colab import drive\n",
        "                platform = 'colab'\n",
        "            except ModuleNotFoundError:\n",
        "                platform = None\n",
        "\n",
        "    print(f'Use: {platform}')\n",
        "    return platform\n",
        "\n",
        "\n",
        "def colab_action():\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive/', force_remount=True)\n",
        "    os.chdir(f'/content/drive/My Drive/LOB/Pipeline')\n",
        "    os.system('pip install automodinit keras_tuner')\n",
        "    os.system('nohup /usr/bin/python3 Colab_saver.py &')\n",
        "\n",
        "\n",
        "def kaggle_action():\n",
        "    ...\n",
        "\n",
        "\n",
        "platform = get_platform()\n",
        "if platform == 'colab':\n",
        "    colab_action()\n",
        "elif platform == 'kaggle':\n",
        "    kaggle_action()\n",
        "\n",
        "import backend as B\n",
        "\n",
        "B.set_backend(platform)\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "\n",
        "from backend import DataBack, ModelBack, DataClass\n",
        "\n",
        "seq_len = 100"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "from models import m_base as test_model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Read saved data, info writen to last_data_info.\n",
            "{'read_from': 'saved', 'proportion': 0.5, 'train_indexes': [0], 'val_indexes': [0], 'test_indexes': []}\n",
            "    Datas:\n",
            "train     : x= (15804, 40)     | y= (15804,)       \n",
            "val       : x= (3951, 40)      | y= (3951,)        \n",
            "    Datasets:\n",
            "train : [158, 100, 40]\n",
            "val   : [39, 100, 40]\n"
          ]
        }
      ],
      "source": [
        "## Load data\n",
        "data_back = DataBack()\n",
        "proportion = input('Data proportion 100-0 in % (press enter for all): ')\n",
        "if proportion == '': proportion = 1\n",
        "else: proportion = float(proportion) / 100\n",
        "\n",
        "train, val, test = data_back.read_saved_data(\n",
        "    proportion=proportion,\n",
        "    train_indexes=[0],\n",
        "    val_indexes=[0],\n",
        ")\n",
        "print(data_back.last_data_info)\n",
        "data_back.inspect_data(train=train, val=val, test=test)\n",
        "\n",
        "ds_train = data_back.data_to_dataset(\n",
        "    data=train,\n",
        "    seq_len=seq_len,\n",
        "    batch_size=100,\n",
        ")\n",
        "ds_val = data_back.data_to_dataset(data=val, seq_len=seq_len, batch_size=100)\n",
        "data_back.inspect_dataset(train=ds_train, val=ds_val)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "convolutional\n",
            "├─ dilation_steps: 4\n",
            "└─ filters: 14\n",
            "feed_forward\n",
            "├─ activation: <function relu at 0x0000027FA6ABADD0>\n",
            "├─ dropout_rate: 0.1\n",
            "├─ kernel_initializer: glorot_uniform\n",
            "├─ kernel_regularizer_class: <class 'keras.regularizers.L2'>\n",
            "├─ out_activation: softmax\n",
            "└─ units: 64\n",
            "optimizer\n",
            "├─ beta_1: 0.9\n",
            "├─ beta_2: 0.999\n",
            "└─ learning_rate: 0.0001\n",
            "seq_len: 100\n",
            "transformer\n",
            "├─ blocks: 2\n",
            "├─ heads: 3\n",
            "└─ share_weights: False\n",
            "\n"
          ]
        }
      ],
      "source": [
        "DEFAULT_PARAMETRS = DataClass(test_model.PARAMETRS)\n",
        "print(DEFAULT_PARAMETRS)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 126,
      "metadata": {},
      "outputs": [],
      "source": [
        "class DataClass:\n",
        "    \"\"\"\n",
        "    make only lover case parametrs and not start with _\n",
        "    All this methods (exept __call__) only for beauty representation :)\n",
        "    \"\"\"\n",
        "    _data_nested={}\n",
        "    _data_expanded={}\n",
        "    \n",
        "    @staticmethod\n",
        "    def __not_data(field=None, get=False, not_data_fields: set = set()):\n",
        "        if not get:\n",
        "            not_data_fields.add(field.__name__)\n",
        "            return field\n",
        "        else:\n",
        "            return not_data_fields\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        target_dict: dict = {},\n",
        "        name: str = '',\n",
        "    ):\n",
        "        for field_name in self.__get_all_fields():\n",
        "            field = getattr(self, field_name)\n",
        "            if type(self.__init__) == type(field):\n",
        "                # TODO: add signature\n",
        "                field_result = field.__func__\n",
        "                setattr(self, field_name, field_result)\n",
        "        self._data_nested_update()\n",
        "        self._data_expanded_update()\n",
        "        \n",
        "    def __new__(\n",
        "        cls,\n",
        "        target_dict: dict = {},\n",
        "        name: str = '',\n",
        "    ):\n",
        "        \"\"\"\n",
        "        build from nested dict\n",
        "        \"\"\"\n",
        "        if target_dict != {}:\n",
        "            new_dataclass = DataClass()\n",
        "            result_dataclass = new_dataclass.__rec_build(name, target_dict)\n",
        "            return result_dataclass\n",
        "\n",
        "        return super().__new__(cls)\n",
        "\n",
        "    def __rec_build(self, field_name: str, field):\n",
        "        if not isinstance(field, dict):\n",
        "            return field\n",
        "\n",
        "        result_dataclass = DataClass()\n",
        "        for inner_field_name, inner_field in field.items():\n",
        "            inner_result = result_dataclass.__rec_build(\n",
        "                inner_field_name,\n",
        "                inner_field,\n",
        "            )\n",
        "            setattr(result_dataclass, inner_field_name, inner_result)\n",
        "        result_dataclass._data_nested_update()\n",
        "        result_dataclass._data_expanded_update()\n",
        "        return result_dataclass\n",
        "\n",
        "    def __call__(self, **kwargs: dict):\n",
        "        \"\"\"\n",
        "        Set up parametrs\n",
        "        \"\"\"\n",
        "        for key, value in kwargs.items():\n",
        "            setattr(self, key, value)\n",
        "\n",
        "    @__not_data\n",
        "    def COPY(self):\n",
        "        return DataClass(self.DATA_NESTED)\n",
        "\n",
        "    def __get_all_fields(self):\n",
        "        filter_func = lambda x: (x[0] != '_') and (x not in self.__not_data(\n",
        "            get=True))\n",
        "        fields = [field for field in self.__dir__() if filter_func(field)]\n",
        "        return fields\n",
        "\n",
        "    def __repr__(self) -> str:\n",
        "        return f'<DataClass object: {[field for field in self.__get_all_fields()]}>'\n",
        "\n",
        "    def __str__(self) -> str:\n",
        "        \"\"\"\n",
        "        Representation of options\n",
        "        \"\"\"\n",
        "        return self.__rec_print()[4:]\n",
        "\n",
        "    def __rec_print(\n",
        "        self,\n",
        "        self_name: str = '',\n",
        "        self_header: str = '',\n",
        "        last=True,\n",
        "    ):\n",
        "        end = \"└─ \"\n",
        "        pipe = \"│  \"\n",
        "        tee = \"├─ \"\n",
        "        blank = \"   \"\n",
        "\n",
        "        result = ''\n",
        "        if not isinstance(self, DataClass):\n",
        "            result = self\n",
        "            # if '<' in repr(self):\n",
        "            #     result = repr(self).split(' at ')[0].replace('<', '').strip()\n",
        "\n",
        "            return f'{self_header}{end if last else tee}{self_name}: {result}\\n'\n",
        "\n",
        "        result = f'{self_header}{end if last else tee}{self_name}\\n'\n",
        "        fields = self.__get_all_fields()\n",
        "        for field_name in fields:\n",
        "            inner_result = DataClass.__rec_print(\n",
        "                getattr(self, field_name),\n",
        "                self_name=field_name,\n",
        "                self_header=f'{self_header}{blank if last else pipe}',\n",
        "                last=field_name == fields[-1])\n",
        "\n",
        "            result += inner_result[6:]\n",
        "\n",
        "        return result\n",
        "\n",
        "    @property\n",
        "    @__not_data\n",
        "    def DATA_NESTED(self):\n",
        "        \"\"\"\n",
        "        Containing options dict\n",
        "        \"\"\"\n",
        "        return self._data_nested\n",
        "\n",
        "    def _data_nested_update(self):\n",
        "        self._data_nested = self.__rec_nest()\n",
        "\n",
        "    def __rec_nest(self, self_name=None):\n",
        "        if not isinstance(self, DataClass):\n",
        "            return {self_name: self}\n",
        "\n",
        "        result = {}\n",
        "        for field_name in self.__get_all_fields():\n",
        "            inner_result = DataClass.__rec_nest(\n",
        "                getattr(self, field_name),\n",
        "                field_name,\n",
        "            )\n",
        "            result.update(inner_result)\n",
        "        return {self_name: result} if self_name is not None else result\n",
        "\n",
        "    @property\n",
        "    @__not_data\n",
        "    def DATA_EXPANDED(self):\n",
        "        return self._data_expanded\n",
        "\n",
        "    def _data_expanded_update(self):\n",
        "        self._data_expanded = {\n",
        "            compound_key.strip()[2:]: value\n",
        "            for value, compound_key in self.__rec_expand()\n",
        "        }\n",
        "\n",
        "    def __rec_expand(self, composite_key=''):\n",
        "        if not isinstance(self, DataClass):\n",
        "            yield (self, composite_key)\n",
        "        else:\n",
        "            for field_name in self.__get_all_fields():\n",
        "                for inner_result in DataClass.__rec_expand(\n",
        "                        getattr(self, field_name),\n",
        "                        str(composite_key) + '__' + str(field_name),\n",
        "                ):\n",
        "                    yield inner_result\n",
        "\n",
        "    def __getitem__(self, value):\n",
        "        if isinstance(value, list):\n",
        "            result = {}\n",
        "            for i_value in value:\n",
        "                result.update({i_value: getattr(self, i_value, None)})\n",
        "            result = DataClass(result)\n",
        "\n",
        "        elif isinstance(value, tuple):\n",
        "            result = self\n",
        "            for i_value in value:\n",
        "                result = getattr(result, i_value, None)\n",
        "        else:\n",
        "            result = getattr(self, value, None)\n",
        "\n",
        "        return result\n",
        "\n",
        "    @__not_data\n",
        "    def COMPARE(self, compared):\n",
        "        return DataClass(self.__rec_compare(compared))\n",
        "\n",
        "    def __rec_compare(self, compared, self_name=None):\n",
        "        if not isinstance(self, DataClass):\n",
        "            return {self_name: (self, compared)}\n",
        "\n",
        "        result = {}\n",
        "        for field_name in self.__get_all_fields():\n",
        "            inner_result = DataClass.__rec_compare(\n",
        "                getattr(self, field_name),\n",
        "                getattr(compared, field_name, None),\n",
        "                field_name,\n",
        "            )\n",
        "            result.update(inner_result)\n",
        "\n",
        "        if self_name is None:\n",
        "            return result\n",
        "        else:\n",
        "            return {self_name: result}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 127,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "v: 2\n",
            "c: {'g': 4, 'h': 7}\n",
            "\n"
          ]
        }
      ],
      "source": [
        "class T(DataClass):\n",
        "    v=2\n",
        "    c={'g':4,'h':7}\n",
        "t =T()\n",
        "print(t)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 129,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{}"
            ]
          },
          "execution_count": 129,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "t.DATA_EXPANDED"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 123,
      "metadata": {},
      "outputs": [
        {
          "ename": "AttributeError",
          "evalue": "'T' object has no attribute '_data_nested'",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[123], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39mprint\u001b[39m(\n\u001b[1;32m----> 2\u001b[0m     DataClass(t\u001b[39m.\u001b[39;49mDATA_NESTED)\n\u001b[0;32m      3\u001b[0m )\n",
            "Cell \u001b[1;32mIn[117], line 121\u001b[0m, in \u001b[0;36mDataClass.DATA_NESTED\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    115\u001b[0m \u001b[39m@property\u001b[39m\n\u001b[0;32m    116\u001b[0m \u001b[39m@__not_data\u001b[39m\n\u001b[0;32m    117\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mDATA_NESTED\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[0;32m    118\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    119\u001b[0m \u001b[39m    Containing options dict\u001b[39;00m\n\u001b[0;32m    120\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 121\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_data_nested\n",
            "\u001b[1;31mAttributeError\u001b[0m: 'T' object has no attribute '_data_nested'"
          ]
        }
      ],
      "source": [
        "print(\n",
        "    DataClass(t.DATA_NESTED)\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 119,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "a\n",
            "└─ d: 3\n",
            "b\n",
            "├─ c: 4\n",
            "└─ e\n",
            "   ├─ v: 5\n",
            "   ├─ c: 4\n",
            "   └─ d\n",
            "      └─ d: 3\n",
            "\n"
          ]
        }
      ],
      "source": [
        "r= {\n",
        "    'a': {\n",
        "        'd': 3\n",
        "    },\n",
        "    \n",
        "    'b': {\n",
        "        'c': 4,\n",
        "        'e': {\n",
        "            'v': 5,\n",
        "            'c': 4,\n",
        "            \n",
        "            'd': {\n",
        "                'd': 3\n",
        "            },\n",
        "        }\n",
        "    }\n",
        "}\n",
        "a = DataClass(r)\n",
        "print(a)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 116,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "v: 5\n",
            "c: 4\n",
            "d\n",
            "└─ d: 3\n",
            "\n"
          ]
        }
      ],
      "source": [
        "print(a['b','e'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 105,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "c: 4\n",
            "e\n",
            "├─ v: 5\n",
            "├─ c: 4\n",
            "└─ d\n",
            "   └─ d: 3\n",
            "\n"
          ]
        }
      ],
      "source": [
        "print(a['b'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 106,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "4.21 µs ± 363 ns per loop (mean ± std. dev. of 7 runs, 100,000 loops each)\n"
          ]
        }
      ],
      "source": [
        "%%timeit\n",
        "a['b']['e']['d']['d']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 113,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "868 µs ± 11.2 µs per loop (mean ± std. dev. of 7 runs, 1,000 loops each)\n"
          ]
        }
      ],
      "source": [
        "%%timeit\n",
        "a[['b','e','d','d']]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 108,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "645 ns ± 12.6 ns per loop (mean ± std. dev. of 7 runs, 1,000,000 loops each)\n"
          ]
        }
      ],
      "source": [
        "%%timeit\n",
        "a.__getattribute__('b').__getattribute__('e').__getattribute__('d').__getattribute__('d')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 109,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "238 ns ± 8.81 ns per loop (mean ± std. dev. of 7 runs, 1,000,000 loops each)\n"
          ]
        }
      ],
      "source": [
        "%%timeit\n",
        "r['b']['e']['d']['d']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "ename": "TypeError",
          "evalue": "optimazer_block() got an unexpected keyword argument 'beta_1'",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[5], line 15\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m     13\u001b[0m     \u001b[39m## Set up parametrs\u001b[39;00m\n\u001b[0;32m     14\u001b[0m     PARAMETRS \u001b[39m=\u001b[39m DEFAULT_PARAMETRS\u001b[39m.\u001b[39mCOPY()\n\u001b[1;32m---> 15\u001b[0m     model \u001b[39m=\u001b[39m test_model\u001b[39m.\u001b[39mblocks\u001b[39m.\u001b[39mbuild_model(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mPARAMETRS\u001b[39m.\u001b[39mDATA_NESTED)\n\u001b[0;32m     16\u001b[0m     train_name \u001b[39m=\u001b[39m ModelBack\u001b[39m.\u001b[39mget_training_name(input_name)\n\u001b[0;32m     17\u001b[0m     \u001b[39mprint\u001b[39m(\n\u001b[0;32m     18\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mPattern model: \u001b[39m\u001b[39m{\u001b[39;00mtest_model\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m,\n\u001b[0;32m     19\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mTrain name: \u001b[39m\u001b[39m{\u001b[39;00mtrain_name\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     22\u001b[0m         sep\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m'\u001b[39m,\n\u001b[0;32m     23\u001b[0m     )\n",
            "File \u001b[1;32md:\\WORKS\\translob\\Pipeline\\models\\m_base.py:543\u001b[0m, in \u001b[0;36mblocks.build_model\u001b[1;34m(seq_len, convolutional, transformer, feed_forward, optimizer)\u001b[0m\n\u001b[0;32m    540\u001b[0m model \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39mkeras\u001b[39m.\u001b[39mModel(inputs\u001b[39m=\u001b[39minputs, outputs\u001b[39m=\u001b[39mx)\n\u001b[0;32m    541\u001b[0m \u001b[39m# Compile\u001b[39;00m\n\u001b[0;32m    542\u001b[0m model\u001b[39m.\u001b[39mcompile(\n\u001b[1;32m--> 543\u001b[0m     blocks\u001b[39m.\u001b[39moptimazer_block(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39moptimizer),\n\u001b[0;32m    544\u001b[0m     loss\u001b[39m=\u001b[39mtf\u001b[39m.\u001b[39mkeras\u001b[39m.\u001b[39mlosses\u001b[39m.\u001b[39mSparseCategoricalCrossentropy(),\n\u001b[0;32m    545\u001b[0m     metrics\u001b[39m=\u001b[39m[\n\u001b[0;32m    546\u001b[0m         tf\u001b[39m.\u001b[39mkeras\u001b[39m.\u001b[39mmetrics\u001b[39m.\u001b[39mSparseCategoricalAccuracy(name\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39msp_acc\u001b[39m\u001b[39m'\u001b[39m),\n\u001b[0;32m    547\u001b[0m         tf\u001b[39m.\u001b[39mkeras\u001b[39m.\u001b[39mmetrics\u001b[39m.\u001b[39mCategoricalAccuracy(name\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39macc\u001b[39m\u001b[39m'\u001b[39m),\n\u001b[0;32m    548\u001b[0m     ],\n\u001b[0;32m    549\u001b[0m )\n\u001b[0;32m    550\u001b[0m \u001b[39mreturn\u001b[39;00m model\n",
            "\u001b[1;31mTypeError\u001b[0m: optimazer_block() got an unexpected keyword argument 'beta_1'"
          ]
        }
      ],
      "source": [
        "## Build\n",
        "tf.keras.backend.clear_session()\n",
        "restore = True if input('Restore? (y-yes, enter-no): ') == 'y' else False\n",
        "input_name = ''\n",
        "while input_name == '':\n",
        "    input_name = input(\n",
        "        f\"Input train name to {'restore' if restore else 'build new'}: \")\n",
        "\n",
        "if restore:\n",
        "    model, train_name = ModelBack.restore_model(input_name)\n",
        "\n",
        "else:\n",
        "    ## Set up parametrs\n",
        "    PARAMETRS = DEFAULT_PARAMETRS.COPY()\n",
        "    model = test_model.blocks.build_model(**PARAMETRS.DATA_NESTED)\n",
        "    train_name = ModelBack.get_training_name(input_name)\n",
        "    print(\n",
        "        f'Pattern model: {test_model.__name__}',\n",
        "        f'Train name: {train_name}',\n",
        "        'Parametrs:',\n",
        "        DEFAULT_PARAMETRS.COMPARE(PARAMETRS),\n",
        "        sep='\\n',\n",
        "    )\n",
        "\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "## Callbacks\n",
        "callback_freq = 1\n",
        "train_dir = f'{ModelBack.callback_path}/{train_name}'\n",
        "callbacks = [\n",
        "    tf.keras.callbacks.TensorBoard(\n",
        "        log_dir=train_dir,\n",
        "        histogram_freq=1,\n",
        "        update_freq=callback_freq,\n",
        "    ),\n",
        "    tf.keras.callbacks.ModelCheckpoint(\n",
        "        f'{train_dir}/checkpoints/' + '{epoch:04d}.keras',\n",
        "        monitor=\"val_sp_acc\",\n",
        "        verbose=0,\n",
        "        save_best_only=False,\n",
        "        save_weights_only=False,\n",
        "        mode=\"auto\",\n",
        "        save_freq=callback_freq,\n",
        "    )\n",
        "]\n",
        "ModelBack.dump(\n",
        "    data_info=data_back.last_data_info,\n",
        "    parametrs=DEFAULT_PARAMETRS.COMPARE(PARAMETRS),\n",
        "    model_path=train_dir,\n",
        ")\n",
        "print(\n",
        "    f\"Callbacks:\\n{[str(type(callback)).split('.')[-1] for callback in callbacks]}\",\n",
        "    f'Directory: {train_dir}',\n",
        "    sep='\\n',\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# %tensorboard"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "## Train\n",
        "training_question = ''\n",
        "while training_question not in ['y', 'n']:\n",
        "    training_question = input(f'Start training now (y-yes) (n-exit): ')\n",
        "if training_question == 'y':\n",
        "    model.fit(\n",
        "        ds_train,\n",
        "        epochs=20,\n",
        "        validation_data=ds_val,\n",
        "        callbacks=callbacks,\n",
        "    )\n"
      ]
    }
  ],
  "metadata": {
    "anaconda-cloud": {},
    "kernelspec": {
      "display_name": "Python 3.10.0 ('zGPU_env': venv)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.0"
    },
    "vscode": {
      "interpreter": {
        "hash": "15c3c8882de147eda8616aa412d6cb2a921cdd19c8088b1f5bdfa95af6065bbb"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
