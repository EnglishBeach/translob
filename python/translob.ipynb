{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b1a70cb7",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mnumpy\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mnp\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtqdm\u001b[39;00m \u001b[39mimport\u001b[39;00m tqdm\n\u001b[1;32m----> 3\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mkeras\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mkeras\u001b[39;00m \u001b[39mimport\u001b[39;00m Input, Model\n\u001b[0;32m      5\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mkeras\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mlayers\u001b[39;00m \u001b[39mimport\u001b[39;00m Layer\n",
      "File \u001b[1;32md:\\WORKS\\translob\\translob_env\\Lib\\site-packages\\keras\\__init__.py:3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39m\"\"\"AUTOGENERATED. DO NOT EDIT.\"\"\"\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mkeras\u001b[39;00m \u001b[39mimport\u001b[39;00m __internal__\n\u001b[0;32m      4\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mkeras\u001b[39;00m \u001b[39mimport\u001b[39;00m activations\n\u001b[0;32m      5\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mkeras\u001b[39;00m \u001b[39mimport\u001b[39;00m applications\n",
      "File \u001b[1;32md:\\WORKS\\translob\\translob_env\\Lib\\site-packages\\keras\\__internal__\\__init__.py:3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39m\"\"\"AUTOGENERATED. DO NOT EDIT.\"\"\"\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mkeras\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39m__internal__\u001b[39;00m \u001b[39mimport\u001b[39;00m backend\n\u001b[0;32m      4\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mkeras\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39m__internal__\u001b[39;00m \u001b[39mimport\u001b[39;00m layers\n\u001b[0;32m      5\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mkeras\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39m__internal__\u001b[39;00m \u001b[39mimport\u001b[39;00m losses\n",
      "File \u001b[1;32md:\\WORKS\\translob\\translob_env\\Lib\\site-packages\\keras\\__internal__\\backend\\__init__.py:3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39m\"\"\"AUTOGENERATED. DO NOT EDIT.\"\"\"\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mkeras\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39msrc\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mbackend\u001b[39;00m \u001b[39mimport\u001b[39;00m _initialize_variables \u001b[39mas\u001b[39;00m initialize_variables\n\u001b[0;32m      4\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mkeras\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39msrc\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mbackend\u001b[39;00m \u001b[39mimport\u001b[39;00m track_variable\n",
      "File \u001b[1;32md:\\WORKS\\translob\\translob_env\\Lib\\site-packages\\keras\\src\\__init__.py:21\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[39m\"\"\"Implementation of the Keras API, the high-level API of TensorFlow.\u001b[39;00m\n\u001b[0;32m     16\u001b[0m \n\u001b[0;32m     17\u001b[0m \u001b[39mDetailed documentation and user guides are available at\u001b[39;00m\n\u001b[0;32m     18\u001b[0m \u001b[39m[keras.io](https://keras.io).\u001b[39;00m\n\u001b[0;32m     19\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m     20\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mkeras\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39msrc\u001b[39;00m \u001b[39mimport\u001b[39;00m distribute\n\u001b[1;32m---> 21\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mkeras\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39msrc\u001b[39;00m \u001b[39mimport\u001b[39;00m models\n\u001b[0;32m     22\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mkeras\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39msrc\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mengine\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39minput_layer\u001b[39;00m \u001b[39mimport\u001b[39;00m Input\n\u001b[0;32m     23\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mkeras\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39msrc\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mengine\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39msequential\u001b[39;00m \u001b[39mimport\u001b[39;00m Sequential\n",
      "File \u001b[1;32md:\\WORKS\\translob\\translob_env\\Lib\\site-packages\\keras\\src\\models\\__init__.py:18\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39m# Copyright 2022 The TensorFlow Authors. All Rights Reserved.\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[39m#\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[39m# Licensed under the Apache License, Version 2.0 (the \"License\");\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[39m# limitations under the License.\u001b[39;00m\n\u001b[0;32m     14\u001b[0m \u001b[39m# ==============================================================================\u001b[39;00m\n\u001b[0;32m     15\u001b[0m \u001b[39m\"\"\"Keras models API.\"\"\"\u001b[39;00m\n\u001b[1;32m---> 18\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mkeras\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39msrc\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mengine\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mfunctional\u001b[39;00m \u001b[39mimport\u001b[39;00m Functional\n\u001b[0;32m     19\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mkeras\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39msrc\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mengine\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39msequential\u001b[39;00m \u001b[39mimport\u001b[39;00m Sequential\n\u001b[0;32m     20\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mkeras\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39msrc\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mengine\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mtraining\u001b[39;00m \u001b[39mimport\u001b[39;00m Model\n",
      "File \u001b[1;32md:\\WORKS\\translob\\translob_env\\Lib\\site-packages\\keras\\src\\engine\\functional.py:23\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mitertools\u001b[39;00m\n\u001b[0;32m     21\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mwarnings\u001b[39;00m\n\u001b[1;32m---> 23\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcompat\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mv2\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mtf\u001b[39;00m\n\u001b[0;32m     25\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mkeras\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39msrc\u001b[39;00m \u001b[39mimport\u001b[39;00m backend\n\u001b[0;32m     26\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mkeras\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39msrc\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mdtensor\u001b[39;00m \u001b[39mimport\u001b[39;00m layout_map \u001b[39mas\u001b[39;00m layout_map_lib\n",
      "File \u001b[1;32md:\\WORKS\\translob\\translob_env\\Lib\\site-packages\\tensorflow\\__init__.py:38\u001b[0m\n\u001b[0;32m     35\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39msys\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39m_sys\u001b[39;00m\n\u001b[0;32m     36\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mtyping\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39m_typing\u001b[39;00m\n\u001b[1;32m---> 38\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpython\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mtools\u001b[39;00m \u001b[39mimport\u001b[39;00m module_util \u001b[39mas\u001b[39;00m _module_util\n\u001b[0;32m     39\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpython\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mutil\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mlazy_loader\u001b[39;00m \u001b[39mimport\u001b[39;00m LazyLoader \u001b[39mas\u001b[39;00m _LazyLoader\n\u001b[0;32m     41\u001b[0m \u001b[39m# Make sure code inside the TensorFlow codebase can use tf2.enabled() at import.\u001b[39;00m\n",
      "File \u001b[1;32md:\\WORKS\\translob\\translob_env\\Lib\\site-packages\\tensorflow\\python\\__init__.py:42\u001b[0m\n\u001b[0;32m     37\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpython\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39meager\u001b[39;00m \u001b[39mimport\u001b[39;00m context\n\u001b[0;32m     39\u001b[0m \u001b[39m# pylint: enable=wildcard-import\u001b[39;00m\n\u001b[0;32m     40\u001b[0m \n\u001b[0;32m     41\u001b[0m \u001b[39m# Bring in subpackages.\u001b[39;00m\n\u001b[1;32m---> 42\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpython\u001b[39;00m \u001b[39mimport\u001b[39;00m data\n\u001b[0;32m     43\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpython\u001b[39;00m \u001b[39mimport\u001b[39;00m distribute\n\u001b[0;32m     44\u001b[0m \u001b[39m# from tensorflow.python import keras\u001b[39;00m\n",
      "File \u001b[1;32md:\\WORKS\\translob\\translob_env\\Lib\\site-packages\\tensorflow\\python\\data\\__init__.py:21\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[39m\"\"\"`tf.data.Dataset` API for input pipelines.\u001b[39;00m\n\u001b[0;32m     16\u001b[0m \n\u001b[0;32m     17\u001b[0m \u001b[39mSee [Importing Data](https://tensorflow.org/guide/data) for an overview.\u001b[39;00m\n\u001b[0;32m     18\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m     20\u001b[0m \u001b[39m# pylint: disable=unused-import\u001b[39;00m\n\u001b[1;32m---> 21\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpython\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mdata\u001b[39;00m \u001b[39mimport\u001b[39;00m experimental\n\u001b[0;32m     22\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpython\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mdata\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mops\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mdataset_ops\u001b[39;00m \u001b[39mimport\u001b[39;00m AUTOTUNE\n\u001b[0;32m     23\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpython\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mdata\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mops\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mdataset_ops\u001b[39;00m \u001b[39mimport\u001b[39;00m Dataset\n",
      "File \u001b[1;32md:\\WORKS\\translob\\translob_env\\Lib\\site-packages\\tensorflow\\python\\data\\experimental\\__init__.py:97\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[39m\"\"\"Experimental API for building input pipelines.\u001b[39;00m\n\u001b[0;32m     16\u001b[0m \n\u001b[0;32m     17\u001b[0m \u001b[39mThis module contains experimental `Dataset` sources and transformations that can\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     93\u001b[0m \u001b[39m@@UNKNOWN_CARDINALITY\u001b[39;00m\n\u001b[0;32m     94\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m     96\u001b[0m \u001b[39m# pylint: disable=unused-import\u001b[39;00m\n\u001b[1;32m---> 97\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpython\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mdata\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mexperimental\u001b[39;00m \u001b[39mimport\u001b[39;00m service\n\u001b[0;32m     98\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpython\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mdata\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mexperimental\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mops\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mbatching\u001b[39;00m \u001b[39mimport\u001b[39;00m dense_to_ragged_batch\n\u001b[0;32m     99\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpython\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mdata\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mexperimental\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mops\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mbatching\u001b[39;00m \u001b[39mimport\u001b[39;00m dense_to_sparse_batch\n",
      "File \u001b[1;32md:\\WORKS\\translob\\translob_env\\Lib\\site-packages\\tensorflow\\python\\data\\experimental\\service\\__init__.py:419\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39m# Copyright 2020 The TensorFlow Authors. All Rights Reserved.\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[39m#\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[39m# Licensed under the Apache License, Version 2.0 (the \"License\");\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[39m# limitations under the License.\u001b[39;00m\n\u001b[0;32m     14\u001b[0m \u001b[39m# ==============================================================================\u001b[39;00m\n\u001b[0;32m     15\u001b[0m \u001b[39m\"\"\"API for using the tf.data service.\u001b[39;00m\n\u001b[0;32m     16\u001b[0m \n\u001b[0;32m     17\u001b[0m \u001b[39mThis module contains:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    416\u001b[0m \u001b[39m  job of ParameterServerStrategy).\u001b[39;00m\n\u001b[0;32m    417\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m--> 419\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpython\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mdata\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mexperimental\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mops\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mdata_service_ops\u001b[39;00m \u001b[39mimport\u001b[39;00m distribute\n\u001b[0;32m    420\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpython\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mdata\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mexperimental\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mops\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mdata_service_ops\u001b[39;00m \u001b[39mimport\u001b[39;00m from_dataset_id\n\u001b[0;32m    421\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpython\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mdata\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mexperimental\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mops\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mdata_service_ops\u001b[39;00m \u001b[39mimport\u001b[39;00m register_dataset\n",
      "File \u001b[1;32md:\\WORKS\\translob\\translob_env\\Lib\\site-packages\\tensorflow\\python\\data\\experimental\\ops\\data_service_ops.py:25\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpython\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mdata\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mexperimental\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mservice\u001b[39;00m \u001b[39mimport\u001b[39;00m _pywrap_server_lib\n\u001b[0;32m     24\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpython\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mdata\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mexperimental\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mservice\u001b[39;00m \u001b[39mimport\u001b[39;00m _pywrap_utils\n\u001b[1;32m---> 25\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpython\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mdata\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mops\u001b[39;00m \u001b[39mimport\u001b[39;00m dataset_ops\n\u001b[0;32m     26\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpython\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mdata\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mops\u001b[39;00m \u001b[39mimport\u001b[39;00m options \u001b[39mas\u001b[39;00m options_lib\n\u001b[0;32m     27\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpython\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mdata\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mops\u001b[39;00m \u001b[39mimport\u001b[39;00m structured_function\n",
      "File \u001b[1;32md:\\WORKS\\translob\\translob_env\\Lib\\site-packages\\tensorflow\\python\\data\\ops\\dataset_ops.py:29\u001b[0m\n\u001b[0;32m     27\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcore\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mprotobuf\u001b[39;00m \u001b[39mimport\u001b[39;00m struct_pb2\n\u001b[0;32m     28\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpython\u001b[39;00m \u001b[39mimport\u001b[39;00m tf2\n\u001b[1;32m---> 29\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpython\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mdata\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mops\u001b[39;00m \u001b[39mimport\u001b[39;00m dataset_autograph\n\u001b[0;32m     30\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpython\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mdata\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mops\u001b[39;00m \u001b[39mimport\u001b[39;00m debug_mode\n\u001b[0;32m     31\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpython\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mdata\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mops\u001b[39;00m \u001b[39mimport\u001b[39;00m iterator_ops\n",
      "File \u001b[1;32md:\\WORKS\\translob\\translob_env\\Lib\\site-packages\\tensorflow\\python\\data\\ops\\dataset_autograph.py:16\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39m# Copyright 2022 The TensorFlow Authors. All Rights Reserved.\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[39m#\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[39m# Licensed under the Apache License, Version 2.0 (the \"License\");\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[39m# limitations under the License.\u001b[39;00m\n\u001b[0;32m     14\u001b[0m \u001b[39m# ==============================================================================\u001b[39;00m\n\u001b[0;32m     15\u001b[0m \u001b[39m\"\"\"Autograph specifc overrides for dataset_ops.\"\"\"\u001b[39;00m\n\u001b[1;32m---> 16\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpython\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mautograph\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39moperators\u001b[39;00m \u001b[39mimport\u001b[39;00m control_flow\n\u001b[0;32m     17\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpython\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mautograph\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39moperators\u001b[39;00m \u001b[39mimport\u001b[39;00m py_builtins\n\u001b[0;32m     18\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpython\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mdata\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mexperimental\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mops\u001b[39;00m \u001b[39mimport\u001b[39;00m take_while_ops\n",
      "File \u001b[1;32md:\\WORKS\\translob\\translob_env\\Lib\\site-packages\\tensorflow\\python\\autograph\\__init__.py:35\u001b[0m\n\u001b[0;32m     33\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpython\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mautograph\u001b[39;00m \u001b[39mimport\u001b[39;00m operators\n\u001b[0;32m     34\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpython\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mautograph\u001b[39;00m \u001b[39mimport\u001b[39;00m utils\n\u001b[1;32m---> 35\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpython\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mautograph\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcore\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mconverter\u001b[39;00m \u001b[39mimport\u001b[39;00m ConversionOptions\n\u001b[0;32m     36\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpython\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mautograph\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcore\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mconverter\u001b[39;00m \u001b[39mimport\u001b[39;00m Feature\n\u001b[0;32m     37\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpython\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mautograph\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mimpl\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mapi\u001b[39;00m \u001b[39mimport\u001b[39;00m AutoGraphError\n",
      "File \u001b[1;32md:\\WORKS\\translob\\translob_env\\Lib\\site-packages\\tensorflow\\python\\autograph\\core\\converter.py:64\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[39m\"\"\"Converter construction support.\u001b[39;00m\n\u001b[0;32m     16\u001b[0m \n\u001b[0;32m     17\u001b[0m \u001b[39mThis module contains a base class for all converters, as well as supporting\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     59\u001b[0m \u001b[39mdependency on AutoGraph.\u001b[39;00m\n\u001b[0;32m     60\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m     62\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39menum\u001b[39;00m\n\u001b[1;32m---> 64\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpython\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mautograph\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpyct\u001b[39;00m \u001b[39mimport\u001b[39;00m anno\n\u001b[0;32m     65\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpython\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mautograph\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpyct\u001b[39;00m \u001b[39mimport\u001b[39;00m ast_util\n\u001b[0;32m     66\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpython\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mautograph\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpyct\u001b[39;00m \u001b[39mimport\u001b[39;00m parser\n",
      "File \u001b[1;32md:\\WORKS\\translob\\translob_env\\Lib\\site-packages\\tensorflow\\python\\autograph\\pyct\\anno.py:24\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39menum\u001b[39;00m\n\u001b[0;32m     22\u001b[0m \u001b[39m# pylint:disable=g-bad-import-order\u001b[39;00m\n\u001b[1;32m---> 24\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mgast\u001b[39;00m\n\u001b[0;32m     25\u001b[0m \u001b[39m# pylint:enable=g-bad-import-order\u001b[39;00m\n\u001b[0;32m     26\u001b[0m \n\u001b[0;32m     27\u001b[0m \n\u001b[0;32m     28\u001b[0m \u001b[39m# TODO(mdan): Shorten the names.\u001b[39;00m\n\u001b[0;32m     29\u001b[0m \u001b[39m# These names are heavily used, and anno.blaa\u001b[39;00m\n\u001b[0;32m     30\u001b[0m \u001b[39m# TODO(mdan): Replace the attr-dict mechanism with a more typed solution.\u001b[39;00m\n\u001b[0;32m     33\u001b[0m \u001b[39mclass\u001b[39;00m \u001b[39mNoValue\u001b[39;00m(enum\u001b[39m.\u001b[39mEnum):\n",
      "File \u001b[1;32md:\\WORKS\\translob\\translob_env\\Lib\\site-packages\\gast\\__init__.py:1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mgast\u001b[39;00m \u001b[39mimport\u001b[39;00m \u001b[39m*\u001b[39m\n\u001b[0;32m      2\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mast\u001b[39;00m \u001b[39mimport\u001b[39;00m NodeVisitor, NodeTransformer, iter_fields, dump\n",
      "File \u001b[1;32md:\\WORKS\\translob\\translob_env\\Lib\\site-packages\\gast\\gast.py:296\u001b[0m\n\u001b[0;32m    294\u001b[0m     \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mast2\u001b[39;00m \u001b[39mimport\u001b[39;00m ast_to_gast, gast_to_ast\n\u001b[0;32m    295\u001b[0m \u001b[39mif\u001b[39;00m _sys\u001b[39m.\u001b[39mversion_info\u001b[39m.\u001b[39mmajor \u001b[39m==\u001b[39m \u001b[39m3\u001b[39m:\n\u001b[1;32m--> 296\u001b[0m     \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mast3\u001b[39;00m \u001b[39mimport\u001b[39;00m ast_to_gast, gast_to_ast\n\u001b[0;32m    299\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mparse\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m    300\u001b[0m     \u001b[39mreturn\u001b[39;00m ast_to_gast(_ast\u001b[39m.\u001b[39mparse(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs))\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:1178\u001b[0m, in \u001b[0;36m_find_and_load\u001b[1;34m(name, import_)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:1149\u001b[0m, in \u001b[0;36m_find_and_load_unlocked\u001b[1;34m(name, import_)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:690\u001b[0m, in \u001b[0;36m_load_unlocked\u001b[1;34m(spec)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap_external>:936\u001b[0m, in \u001b[0;36mexec_module\u001b[1;34m(self, module)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap_external>:1032\u001b[0m, in \u001b[0;36mget_code\u001b[1;34m(self, fullname)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap_external>:1131\u001b[0m, in \u001b[0;36mget_data\u001b[1;34m(self, path)\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import keras\n",
    "from keras import Input, Model\n",
    "from keras.layers import Layer\n",
    "from keras import backend as K\n",
    "from keras import layers\n",
    "from keras.preprocessing import timeseries_dataset_from_array\n",
    "\n",
    "# from sklearn.metrics import accuracy_score, classification_report\n",
    "# from keras.callbacks import ModelCheckpoint\n",
    "\n",
    "from LobTransformer import TransformerBlock"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01beb6e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "\n",
    "# download FI2010 dataset from \n",
    "# https://etsin.fairdata.fi/dataset/73eb48d7-4dbc-4a10-a52a-da745b47a649\n",
    "FI2010_DIR = r'D:\\WORKS\\translob\\dataset\\BenchmarkDatasets'\n",
    "\n",
    "def gen_data(data, horizon):\n",
    "    x = data[:40, :].T  # 40 == 10 price + volume asks + 10 price + volume bids\n",
    "    # FIXME: delete .T\n",
    "    y = data[-5 + horizon, :].T  # 5\n",
    "    return x[:-1], (y[1:] - 1).astype(np.int32)  # shift y by 1\n",
    "\n",
    "\n",
    "def load_dataset(horizon):\n",
    "    dec_data = np.loadtxt(\n",
    "        f'{FI2010_DIR}/NoAuction/1.NoAuction_Zscore/NoAuction_Zscore_Training/Train_Dst_NoAuction_ZScore_CF_7.txt'\n",
    "    )\n",
    "    dec_train = dec_data[:, :int(np.floor(dec_data.shape[1] * 0.8))]\n",
    "    dec_val = dec_data[:, int(np.floor(dec_data.shape[1] * 0.8)):]\n",
    "\n",
    "    dec_test1 = np.loadtxt(\n",
    "        f'{FI2010_DIR}/NoAuction/1.NoAuction_Zscore/NoAuction_Zscore_Testing/Test_Dst_NoAuction_ZScore_CF_7.txt'\n",
    "    )\n",
    "    dec_test2 = np.loadtxt(\n",
    "        f'{FI2010_DIR}/NoAuction/1.NoAuction_Zscore/NoAuction_Zscore_Testing/Test_Dst_NoAuction_ZScore_CF_8.txt'\n",
    "    )\n",
    "    dec_test3 = np.loadtxt(\n",
    "        f'{FI2010_DIR}/NoAuction/1.NoAuction_Zscore/NoAuction_Zscore_Testing/Test_Dst_NoAuction_ZScore_CF_9.txt'\n",
    "    )\n",
    "    dec_test = np.hstack((dec_test1, dec_test2, dec_test3))\n",
    "    result = (\n",
    "        gen_data(dec_train,horizon), \n",
    "        gen_data(dec_val,horizon), \n",
    "        gen_data(dec_test, horizon)) #yapf: disable\n",
    "    return result\n",
    "\n",
    "(x_train, y_train), (x_val, y_val), (x_test, y_test) = load_dataset(horizon=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ef29505",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test of fi2020 module from start\n",
    "# a = np.array([\n",
    "#     2615, 353, 2618, 200, 2619, 164, 2620, 532, 2621, 151, 2623, 837, 2625,\n",
    "#     150, 2626, 787, 2629, 146, 2633, 311, 2606, 326, 2604, 682, 2602, 786,\n",
    "#     2600, 893, 2599, 159, 2595, 100, 2593, 143, 2591, 134, 2588, 123, 2579, 128\n",
    "# ])\n",
    "# x_train[0]*a+a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6eef9034",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "class PositionalEncodingLayer(Layer):\n",
    "\n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "\n",
    "    def call(self, x, *args, **kwargs):\n",
    "        steps, d_model = x.get_shape()[-2:]\n",
    "        global A \n",
    "        A= x,steps,d_model\n",
    "        ps = np.zeros([steps, 1], dtype=K.floatx())\n",
    "        for step in range(steps):\n",
    "            ps[step, :] = [(2 / (steps - 1)) * step - 1]\n",
    "\n",
    "        ps_expand = K.expand_dims(K.constant(ps), axis=0)\n",
    "        ps_tiled = K.tile(ps_expand, [K.shape(x)[0], 1, 1])\n",
    "\n",
    "        x = K.concatenate([x, ps_tiled], axis=-1)\n",
    "        return x\n",
    "\n",
    "\n",
    "def eval(model, X_test, y_test, **kwargs):\n",
    "    ts = TimeseriesGenerator(X_test,\n",
    "                             y_test,\n",
    "                             kwargs.get('sequence_length', 100),\n",
    "                             batch_size=32,\n",
    "                             shuffle=False)\n",
    "    y_true = np.concatenate([y for x, y in ts])\n",
    "    y_pred = np.argmax(model.predict(ts), -1)\n",
    "    print(classification_report(y_true, y_pred))\n",
    "    return classification_report(y_true, y_pred,\n",
    "                                 output_dict=True)['weighted avg']['f1-score']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb99e75a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class OptionsClass:\n",
    "    \"\"\"\n",
    "    make only lover case parametrs and not start with _\n",
    "    All this methods (exept __call__) only for beauty representation :)\n",
    "    \"\"\"\n",
    "    _unpack_dicts_ = False\n",
    "\n",
    "    def __call__(self, **kwargs: dict):\n",
    "        \"\"\"\n",
    "        Set up parametrs\n",
    "        \"\"\"\n",
    "        for key, value in kwargs.items():\n",
    "            self.__setattr__(key, value)\n",
    "\n",
    "    def __repr__(self):\n",
    "        key_list = []\n",
    "        for composite_key, value in self.Options.items():\n",
    "            add_str = f'{composite_key}: '\n",
    "\n",
    "            if isinstance(value, np.ndarray | range):\n",
    "                value_str = f'[{value[0]}, {value[1]} .. {value[-2]}, {value[-1]}]; len = {len(value)}'\n",
    "\n",
    "            else:\n",
    "                value_str = f'{value}'\n",
    "\n",
    "            key_list.append(add_str + value_str)\n",
    "        return str('\\n'.join(key_list))\n",
    "\n",
    "    @property\n",
    "    def Options(self):\n",
    "        \"\"\"\n",
    "        Containing options dict\n",
    "        \"\"\"\n",
    "        return {\n",
    "            compound_key.strip(): value\n",
    "            for value, compound_key in self._recursion_view()\n",
    "        }\n",
    "\n",
    "    def _all_options(self):\n",
    "        data_filter = lambda x: (x[0] != '_') and x[0].islower()\n",
    "        options = list(filter(data_filter, self.__dir__()))\n",
    "        return {option: self.__getattribute__(option) for option in options}\n",
    "\n",
    "    def _recursion_view(self, composite_key=''):\n",
    "        \"\"\"\n",
    "        Need for recursion representation containing options\n",
    "        \"\"\"\n",
    "        dilimiter = '  '\n",
    "        if isinstance(self, OptionsClass):\n",
    "            for key in self._all_options():\n",
    "                for inner_key in OptionsClass._recursion_view(\n",
    "                        self.__getattribute__(key),\n",
    "                        str(composite_key) + dilimiter + str(key),\n",
    "                ):\n",
    "                    yield inner_key\n",
    "\n",
    "        # Dicts store as dicts and include in composite key\n",
    "        # (to unpack dicts - uncomment below):\n",
    "\n",
    "        elif isinstance(self, dict) and OptionsClass._unpack_dicts_:\n",
    "            for key in self:\n",
    "                for inner_key in OptionsClass._recursion_view(\n",
    "                        self[key],\n",
    "                        str(composite_key) + dilimiter + str(key),\n",
    "                ):\n",
    "                    yield inner_key\n",
    "\n",
    "        else:\n",
    "            yield (self, composite_key)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bad9525",
   "metadata": {},
   "outputs": [],
   "source": [
    " # def __str__(self):\n",
    "    #     key_list = []\n",
    "    #     options= {\n",
    "    #         compound_key.strip(): value\n",
    "    #         for value, compound_key in self._recursion_view()\n",
    "    #     }\n",
    "    #     for composite_key, value in options.items():\n",
    "    #         add_str = f'{composite_key}: '\n",
    "\n",
    "    #         if isinstance(value, np.ndarray | range):\n",
    "    #             value_str = f'[{value[0]}, {value[1]} .. {value[-2]}, {value[-1]}]; len = {len(value)}'\n",
    "\n",
    "    #         else:\n",
    "    #             value_str = f'{value}'\n",
    "\n",
    "    #         key_list.append(add_str + value_str)\n",
    "    #     return str('\\n'.join(key_list))\n",
    "        \n",
    "    # def _recursion_view(self, composite_key=''):\n",
    "        # \"\"\"\n",
    "        # Need for recursion representation containing options\n",
    "        # \"\"\"\n",
    "        # dilimiter = '  '\n",
    "        # if isinstance(self, OptionsClass):\n",
    "        #     for key in self._get_all_options():\n",
    "        #         for inner_key in OptionsClass._recursion_view(\n",
    "        #                 self.__getattribute__(key),\n",
    "        #                 str(composite_key) + dilimiter + str(key),\n",
    "        #         ):\n",
    "        #             yield inner_key\n",
    "\n",
    "        # # Dicts store as dicts and include in composite key\n",
    "        # # (to unpack dicts - uncomment below):\n",
    "\n",
    "        # elif isinstance(self, dict) and OptionsClass._unpack_dicts_:\n",
    "        #     for key in self:\n",
    "        #         for inner_key in OptionsClass._recursion_view(\n",
    "        #                 self[key],\n",
    "        #                 str(composite_key) + dilimiter + str(key),\n",
    "        #         ):\n",
    "        #             yield inner_key\n",
    "\n",
    "        # else:\n",
    "        #     yield (self, composite_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "3eaed67b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataClass:\n",
    "    \"\"\"\n",
    "    make only lover case parametrs and not start with _\n",
    "    All this methods (exept __call__) only for beauty representation :)\n",
    "    \"\"\"\n",
    "    _data_filter = lambda x: (x[0] != '_') and ('Data' not in x)\n",
    "\n",
    "    def __new__(\n",
    "        cls,\n",
    "        target_dict: dict = None,\n",
    "        name: str='',\n",
    "    ):\n",
    "        if target_dict is not None:\n",
    "            result =DataClass()\n",
    "            return result._rec_build_(name, target_dict)\n",
    "        return super().__new__(cls)\n",
    "    \n",
    "    def __call__(self, **kwargs: dict):\n",
    "        \"\"\"\n",
    "        Set up parametrs\n",
    "        \"\"\"\n",
    "        for key, value in kwargs.items():\n",
    "            self.__setattr__(key, value)\n",
    "\n",
    "    def __repr__(self) -> str:\n",
    "        \"\"\"\n",
    "        Representation of options\n",
    "        \"\"\"\n",
    "        return DataClass._rec_print_(self.Data)\n",
    "\n",
    "    @property\n",
    "    def Data(self):\n",
    "        \"\"\"\n",
    "        Containing options dict\n",
    "        \"\"\"\n",
    "        return self._rec_dict_()\n",
    "\n",
    "    def _get_all_options(self):\n",
    "        # Add except fields\n",
    "\n",
    "        options = list(filter(DataClass._data_filter, self.__dir__()))\n",
    "        return options\n",
    "\n",
    "    def _rec_dict_(self, self_name=None):\n",
    "        if not isinstance(self, DataClass):\n",
    "            return {self_name: self}\n",
    "\n",
    "        result = {}\n",
    "        for option in self._get_all_options():\n",
    "\n",
    "            inner_dict = DataClass._rec_dict_(\n",
    "                self.__getattribute__(option),\n",
    "                option,\n",
    "            )\n",
    "            result.update(inner_dict)\n",
    "\n",
    "        if self_name is None:\n",
    "            return result\n",
    "        else:\n",
    "            return {self_name: result}\n",
    "\n",
    "    @staticmethod\n",
    "    def _rec_print_(target, margin: str = ''):\n",
    "        if not isinstance(target, dict):\n",
    "            return f'{target}'\n",
    "        result = margin\n",
    "\n",
    "        for key, value in target.items():\n",
    "            inner_string = DataClass._rec_print_(\n",
    "                value,\n",
    "                margin + ' ' * 4,\n",
    "            )\n",
    "            result += '\\n' + margin + f'{key}: {inner_string}'\n",
    "\n",
    "        if margin == '':\n",
    "            return result[1:]\n",
    "        else:\n",
    "            return result\n",
    "\n",
    "    def _rec_build_(self, self_name: str, target):\n",
    "        if not isinstance(target, dict):\n",
    "            self.__setattr__(self_name, target)\n",
    "            return\n",
    "        \n",
    "        result = DataClass()\n",
    "        self.__setattr__(self_name, result)\n",
    "        \n",
    "\n",
    "        for name, inner_target in target.items():\n",
    "            inner_result = result._rec_build_(\n",
    "                name,\n",
    "                inner_target,\n",
    "            )\n",
    "            if inner_result is not None:\n",
    "                self.__setattr__(self_name, inner_result)\n",
    "        return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "2c504571",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model parametrs\n",
    "seq_len = 100\n",
    "\n",
    "class CN_pars(DataClass):\n",
    "    n_filters=14\n",
    "    dilation_steps=4  # dilation = 2**dilation_step\n",
    "\n",
    "class AN_pars(DataClass):\n",
    "    attention_heads = 1\n",
    "    blocks = 2\n",
    "    share_weights = True\n",
    "\n",
    "class FF_pars(DataClass):\n",
    "    dropout_rate = 0.1\n",
    "\n",
    "class Optimaser_pars(DataClass):\n",
    "    lr = 0.0001\n",
    "    adam_beta1 = 0.9\n",
    "    adam_beta2 = 0.999\n",
    "\n",
    "class Trainig_pars(DataClass):\n",
    "    batch_size = 512\n",
    "    epochs = 150\n",
    "    \n",
    "class Full_pars(DataClass):\n",
    "    cn= CN_pars()\n",
    "    an = AN_pars()\n",
    "    ff=FF_pars()\n",
    "    optimazer=Optimaser_pars()\n",
    "    training=Trainig_pars()\n",
    "    \n",
    "pars= Full_pars() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "2c504571",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model parametrs\n",
    "seq_len = 100\n",
    "\n",
    "cn_pars1 = dict(\n",
    "    n_filters=14,\n",
    "    dilation_steps=4,\n",
    ")  # dilation = 2**dilation_step\n",
    "\n",
    "# AN\n",
    "attention_heads = 1\n",
    "blocks = 2\n",
    "share_weights = True\n",
    "\n",
    "# FF\n",
    "dropout_rate = 0.1\n",
    "\n",
    "# Optimizer\n",
    "lr = 0.0001\n",
    "adam_beta1 = 0.9\n",
    "adam_beta2 = 0.999\n",
    "\n",
    "# Training\n",
    "batch_size = 512\n",
    "epochs = 150"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c83adfaf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                Output Shape                 Param #   Connected to                  \n",
      "==================================================================================================\n",
      " input_1 (InputLayer)        [(None, 100, 40)]            0         []                            \n",
      "                                                                                                  \n",
      " conv1d (Conv1D)             (None, 100, 14)              1134      ['input_1[0][0]']             \n",
      "                                                                                                  \n",
      " conv1d_1 (Conv1D)           (None, 100, 14)              406       ['conv1d[0][0]']              \n",
      "                                                                                                  \n",
      " conv1d_2 (Conv1D)           (None, 100, 14)              406       ['conv1d_1[0][0]']            \n",
      "                                                                                                  \n",
      " conv1d_3 (Conv1D)           (None, 100, 14)              406       ['conv1d_2[0][0]']            \n",
      "                                                                                                  \n",
      " conv1d_4 (Conv1D)           (None, 100, 14)              406       ['conv1d_3[0][0]']            \n",
      "                                                                                                  \n",
      " layer_normalization (Layer  (None, 100, 14)              28        ['conv1d_4[0][0]']            \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " positional_encoding_layer   (None, 100, 15)              0         ['layer_normalization[0][0]'] \n",
      " (PositionalEncodingLayer)                                                                        \n",
      "                                                                                                  \n",
      " transformer_block (Transfo  (None, 100, 15)              2610      ['positional_encoding_layer[0]\n",
      " rmerBlock)                                                         [0]',                         \n",
      "                                                                     'transformer_block[0][0]']   \n",
      "                                                                                                  \n",
      " flatten (Flatten)           (None, 1500)                 0         ['transformer_block[1][0]']   \n",
      "                                                                                                  \n",
      " dense (Dense)               (None, 64)                   96064     ['flatten[0][0]']             \n",
      "                                                                                                  \n",
      " dropout (Dropout)           (None, 64)                   0         ['dense[0][0]']               \n",
      "                                                                                                  \n",
      " dense_1 (Dense)             (None, 3)                    195       ['dropout[0][0]']             \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 101655 (397.09 KB)\n",
      "Trainable params: 101655 (397.09 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "__________________________________________________________________________________________________\n",
      "Train (203799, 40) (203799,) Val (50949, 40) (50949,)\n"
     ]
    }
   ],
   "source": [
    "# Model\n",
    "\n",
    "# Input\n",
    "inputs = Input(shape=(seq_len, 40))\n",
    "x = inputs\n",
    "\n",
    "# CN\n",
    "dilation_steps = [2**dilation for dilation in range(pars.cn.dilation_steps + 1)]\n",
    "for dilation in dilation_steps:\n",
    "    x = layers.Conv1D(\n",
    "        filters=pars.cn.n_filters,\n",
    "        kernel_size=2,\n",
    "        dilation_rate=dilation,\n",
    "        activation='relu',\n",
    "        padding='causal',\n",
    "    )(x)\n",
    "\n",
    "norm = layers.LayerNormalization()(x)\n",
    "x=norm\n",
    "pos = PositionalEncodingLayer()(x)\n",
    "x = pos\n",
    "tb = TransformerBlock(\n",
    "    'transformer_block',\n",
    "    pars.an.attention_heads,\n",
    "    True,\n",
    ")\n",
    "for block in range(pars.an.blocks):\n",
    "    if share_weights:\n",
    "        x = tb(x)\n",
    "    else:\n",
    "        x = TransformerBlock(\n",
    "            f'transformer_block_{block}',\n",
    "            pars.an.attention_heads,\n",
    "            True,\n",
    "        )(x)\n",
    "\n",
    "x = layers.Flatten()(x)\n",
    "x = layers.Dense(64,\n",
    "                 activation='relu',\n",
    "                 kernel_regularizer='l2',\n",
    "                 kernel_initializer='glorot_uniform')(x)\n",
    "x = layers.Dropout(dropout_rate)(x)\n",
    "out = layers.Dense(3, activation='softmax')(x)\n",
    "model = Model(inputs=inputs, outputs=out)\n",
    "model.summary()\n",
    "\n",
    "model.compile(\n",
    "    keras.optimizers.Adam(\n",
    "        learning_rate=lr,\n",
    "        beta_1=adam_beta1,\n",
    "        beta_2=adam_beta2,\n",
    "        name=\"Adam\",\n",
    "    ),\n",
    "    loss=keras.losses.SparseCategoricalCrossentropy(),\n",
    "    metrics=['sparse_categorical_accuracy'],\n",
    ")\n",
    "print(\n",
    "    'Train',\n",
    "    x_train.shape,\n",
    "    y_train.shape,\n",
    "    'Val',\n",
    "    x_val.shape,\n",
    "    y_val.shape,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "a2b00f29",
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_shape(value_x,value_y):\n",
    "    value_x.set_shape((None,100,40))\n",
    "    # value_y.set_shape((None,))\n",
    "    return value_x,value_y\n",
    "\n",
    "ds_train = timeseries_dataset_from_array(\n",
    "    data=x_train,\n",
    "    targets=y_train,\n",
    "    batch_size=batch_size,\n",
    "    sequence_length=seq_len,\n",
    "    shuffle=True,\n",
    ")\n",
    "ds_train= ds_train.map(set_shape)\n",
    "\n",
    "ds_val = timeseries_dataset_from_array(\n",
    "    data=x_val,\n",
    "    targets=y_val,\n",
    "    batch_size=batch_size,\n",
    "    sequence_length=seq_len,\n",
    "    shuffle=True,\n",
    ")\n",
    "ds_val= ds_val.map(set_shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "73c8bb4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/150\n",
      " 28/398 [=>............................] - ETA: 3:18 - loss: 1.1548 - sparse_categorical_accuracy: 0.4059"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[55], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m model\u001b[39m.\u001b[39;49mfit(\n\u001b[0;32m      2\u001b[0m     ds_train,\n\u001b[0;32m      3\u001b[0m     batch_size\u001b[39m=\u001b[39;49mbatch_size,\n\u001b[0;32m      4\u001b[0m     epochs\u001b[39m=\u001b[39;49mepochs,\n\u001b[0;32m      5\u001b[0m     validation_data\u001b[39m=\u001b[39;49mds_val,\n\u001b[0;32m      6\u001b[0m )\n",
      "File \u001b[1;32md:\\WORKS\\translob\\translob_env\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     63\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m     64\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m---> 65\u001b[0m     \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m     66\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32md:\\WORKS\\translob\\translob_env\\Lib\\site-packages\\keras\\src\\engine\\training.py:1742\u001b[0m, in \u001b[0;36mModel.fit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1734\u001b[0m \u001b[39mwith\u001b[39;00m tf\u001b[39m.\u001b[39mprofiler\u001b[39m.\u001b[39mexperimental\u001b[39m.\u001b[39mTrace(\n\u001b[0;32m   1735\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mtrain\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m   1736\u001b[0m     epoch_num\u001b[39m=\u001b[39mepoch,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1739\u001b[0m     _r\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m,\n\u001b[0;32m   1740\u001b[0m ):\n\u001b[0;32m   1741\u001b[0m     callbacks\u001b[39m.\u001b[39mon_train_batch_begin(step)\n\u001b[1;32m-> 1742\u001b[0m     tmp_logs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtrain_function(iterator)\n\u001b[0;32m   1743\u001b[0m     \u001b[39mif\u001b[39;00m data_handler\u001b[39m.\u001b[39mshould_sync:\n\u001b[0;32m   1744\u001b[0m         context\u001b[39m.\u001b[39masync_wait()\n",
      "File \u001b[1;32md:\\WORKS\\translob\\translob_env\\Lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    148\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m    149\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 150\u001b[0m   \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m    151\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m    152\u001b[0m   filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32md:\\WORKS\\translob\\translob_env\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\polymorphic_function.py:825\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    822\u001b[0m compiler \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mxla\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jit_compile \u001b[39melse\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mnonXla\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    824\u001b[0m \u001b[39mwith\u001b[39;00m OptionalXlaContext(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jit_compile):\n\u001b[1;32m--> 825\u001b[0m   result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwds)\n\u001b[0;32m    827\u001b[0m new_tracing_count \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mexperimental_get_tracing_count()\n\u001b[0;32m    828\u001b[0m without_tracing \u001b[39m=\u001b[39m (tracing_count \u001b[39m==\u001b[39m new_tracing_count)\n",
      "File \u001b[1;32md:\\WORKS\\translob\\translob_env\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\polymorphic_function.py:857\u001b[0m, in \u001b[0;36mFunction._call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    854\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock\u001b[39m.\u001b[39mrelease()\n\u001b[0;32m    855\u001b[0m   \u001b[39m# In this case we have created variables on the first call, so we run the\u001b[39;00m\n\u001b[0;32m    856\u001b[0m   \u001b[39m# defunned version which is guaranteed to never create variables.\u001b[39;00m\n\u001b[1;32m--> 857\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_no_variable_creation_fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwds)  \u001b[39m# pylint: disable=not-callable\u001b[39;00m\n\u001b[0;32m    858\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_variable_creation_fn \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    859\u001b[0m   \u001b[39m# Release the lock early so that multiple threads can perform the call\u001b[39;00m\n\u001b[0;32m    860\u001b[0m   \u001b[39m# in parallel.\u001b[39;00m\n\u001b[0;32m    861\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock\u001b[39m.\u001b[39mrelease()\n",
      "File \u001b[1;32md:\\WORKS\\translob\\translob_env\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\tracing_compiler.py:148\u001b[0m, in \u001b[0;36mTracingCompiler.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    145\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock:\n\u001b[0;32m    146\u001b[0m   (concrete_function,\n\u001b[0;32m    147\u001b[0m    filtered_flat_args) \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_maybe_define_function(args, kwargs)\n\u001b[1;32m--> 148\u001b[0m \u001b[39mreturn\u001b[39;00m concrete_function\u001b[39m.\u001b[39;49m_call_flat(\n\u001b[0;32m    149\u001b[0m     filtered_flat_args, captured_inputs\u001b[39m=\u001b[39;49mconcrete_function\u001b[39m.\u001b[39;49mcaptured_inputs)\n",
      "File \u001b[1;32md:\\WORKS\\translob\\translob_env\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\monomorphic_function.py:1349\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[1;34m(self, args, captured_inputs)\u001b[0m\n\u001b[0;32m   1345\u001b[0m possible_gradient_type \u001b[39m=\u001b[39m gradients_util\u001b[39m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[0;32m   1346\u001b[0m \u001b[39mif\u001b[39;00m (possible_gradient_type \u001b[39m==\u001b[39m gradients_util\u001b[39m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[0;32m   1347\u001b[0m     \u001b[39mand\u001b[39;00m executing_eagerly):\n\u001b[0;32m   1348\u001b[0m   \u001b[39m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[1;32m-> 1349\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_build_call_outputs(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_inference_function(\u001b[39m*\u001b[39;49margs))\n\u001b[0;32m   1350\u001b[0m forward_backward \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[0;32m   1351\u001b[0m     args,\n\u001b[0;32m   1352\u001b[0m     possible_gradient_type,\n\u001b[0;32m   1353\u001b[0m     executing_eagerly)\n\u001b[0;32m   1354\u001b[0m forward_function, args_with_tangents \u001b[39m=\u001b[39m forward_backward\u001b[39m.\u001b[39mforward()\n",
      "File \u001b[1;32md:\\WORKS\\translob\\translob_env\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\atomic_function.py:196\u001b[0m, in \u001b[0;36mAtomicFunction.__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m    194\u001b[0m \u001b[39mwith\u001b[39;00m record\u001b[39m.\u001b[39mstop_recording():\n\u001b[0;32m    195\u001b[0m   \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_bound_context\u001b[39m.\u001b[39mexecuting_eagerly():\n\u001b[1;32m--> 196\u001b[0m     outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_bound_context\u001b[39m.\u001b[39;49mcall_function(\n\u001b[0;32m    197\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mname,\n\u001b[0;32m    198\u001b[0m         \u001b[39mlist\u001b[39;49m(args),\n\u001b[0;32m    199\u001b[0m         \u001b[39mlen\u001b[39;49m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfunction_type\u001b[39m.\u001b[39;49mflat_outputs),\n\u001b[0;32m    200\u001b[0m     )\n\u001b[0;32m    201\u001b[0m   \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    202\u001b[0m     outputs \u001b[39m=\u001b[39m make_call_op_in_graph(\u001b[39mself\u001b[39m, \u001b[39mlist\u001b[39m(args))\n",
      "File \u001b[1;32md:\\WORKS\\translob\\translob_env\\Lib\\site-packages\\tensorflow\\python\\eager\\context.py:1457\u001b[0m, in \u001b[0;36mContext.call_function\u001b[1;34m(self, name, tensor_inputs, num_outputs)\u001b[0m\n\u001b[0;32m   1455\u001b[0m cancellation_context \u001b[39m=\u001b[39m cancellation\u001b[39m.\u001b[39mcontext()\n\u001b[0;32m   1456\u001b[0m \u001b[39mif\u001b[39;00m cancellation_context \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m-> 1457\u001b[0m   outputs \u001b[39m=\u001b[39m execute\u001b[39m.\u001b[39;49mexecute(\n\u001b[0;32m   1458\u001b[0m       name\u001b[39m.\u001b[39;49mdecode(\u001b[39m\"\u001b[39;49m\u001b[39mutf-8\u001b[39;49m\u001b[39m\"\u001b[39;49m),\n\u001b[0;32m   1459\u001b[0m       num_outputs\u001b[39m=\u001b[39;49mnum_outputs,\n\u001b[0;32m   1460\u001b[0m       inputs\u001b[39m=\u001b[39;49mtensor_inputs,\n\u001b[0;32m   1461\u001b[0m       attrs\u001b[39m=\u001b[39;49mattrs,\n\u001b[0;32m   1462\u001b[0m       ctx\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m,\n\u001b[0;32m   1463\u001b[0m   )\n\u001b[0;32m   1464\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m   1465\u001b[0m   outputs \u001b[39m=\u001b[39m execute\u001b[39m.\u001b[39mexecute_with_cancellation(\n\u001b[0;32m   1466\u001b[0m       name\u001b[39m.\u001b[39mdecode(\u001b[39m\"\u001b[39m\u001b[39mutf-8\u001b[39m\u001b[39m\"\u001b[39m),\n\u001b[0;32m   1467\u001b[0m       num_outputs\u001b[39m=\u001b[39mnum_outputs,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1471\u001b[0m       cancellation_manager\u001b[39m=\u001b[39mcancellation_context,\n\u001b[0;32m   1472\u001b[0m   )\n",
      "File \u001b[1;32md:\\WORKS\\translob\\translob_env\\Lib\\site-packages\\tensorflow\\python\\eager\\execute.py:53\u001b[0m, in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     51\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m     52\u001b[0m   ctx\u001b[39m.\u001b[39mensure_initialized()\n\u001b[1;32m---> 53\u001b[0m   tensors \u001b[39m=\u001b[39m pywrap_tfe\u001b[39m.\u001b[39;49mTFE_Py_Execute(ctx\u001b[39m.\u001b[39;49m_handle, device_name, op_name,\n\u001b[0;32m     54\u001b[0m                                       inputs, attrs, num_outputs)\n\u001b[0;32m     55\u001b[0m \u001b[39mexcept\u001b[39;00m core\u001b[39m.\u001b[39m_NotOkStatusException \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m     56\u001b[0m   \u001b[39mif\u001b[39;00m name \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model.fit(\n",
    "    ds_train,\n",
    "    batch_size=batch_size,\n",
    "    epochs=epochs,\n",
    "    validation_data=ds_val,\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11.1 ('translob_env': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  },
  "vscode": {
   "interpreter": {
    "hash": "9a9191c6b2d33302590e376f8aee71d3c7e87a446da0bda5474eb7c47655c891"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
