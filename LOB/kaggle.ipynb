{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from keras.preprocessing import timeseries_dataset_from_array as _timeseries_dataset_from_array\n",
    "\n",
    "# download FI2010 dataset from\n",
    "# https://etsin.fairdata.fi/dataset/73eb48d7-4dbc-4a10-a52a-da745b47a649\n",
    "_FI2010_DIR_ = r'D:\\WORKS\\translob\\dataset\\BenchmarkDatasets'\n",
    "_add_path_ = r'/NoAuction/1.NoAuction_Zscore/NoAuction_Zscore'\n",
    "\n",
    "PATH = _FI2010_DIR_ + _add_path_\n",
    "\n",
    "\n",
    "# Save\n",
    "\n",
    "    \n",
    "def save_data(x, y, name):\n",
    "    \"\"\"\n",
    "    kinds = 'test', 'train', 'val'\n",
    "    \"\"\"\n",
    "    with open(f'saved_data/x_{name}.npy', 'wb') as file:\n",
    "        np.save(file, x)\n",
    "    with open(f'saved_data/y_{name}.npy', 'wb') as file:\n",
    "        np.save(file, y)\n",
    "\n",
    "\n",
    "# Load data\n",
    "def _gen_data(data, horizon):\n",
    "    x = data[:40, :].T  # 40 == 10 price + volume asks + 10 price + volume bids\n",
    "    y = data[-5 + horizon, :].T  # 5\n",
    "    return [x[:-1], (y[1:] - 1).astype(np.int32)]  # shift y by 1\n",
    "\n",
    "\n",
    "def load_datas(horizon):\n",
    "    dec_data = np.loadtxt(\n",
    "        f'{PATH}_Training/Train_Dst_NoAuction_ZScore_CF_7.txt')\n",
    "\n",
    "    dec_train = dec_data[:, :int(np.floor(dec_data.shape[1] * 0.8))]\n",
    "    dec_val = dec_data[:, int(np.floor(dec_data.shape[1] * 0.8)):]\n",
    "\n",
    "    dec_test1 = np.loadtxt(\n",
    "        f'{PATH}_Testing/Test_Dst_NoAuction_ZScore_CF_7.txt')\n",
    "\n",
    "    dec_test2 = np.loadtxt(\n",
    "        f'{PATH}_Testing/Test_Dst_NoAuction_ZScore_CF_8.txt')\n",
    "\n",
    "    dec_test3 = np.loadtxt(\n",
    "        f'{PATH}_Testing/Test_Dst_NoAuction_ZScore_CF_9.txt')\n",
    "\n",
    "    dec_test = np.hstack((dec_test1, dec_test2, dec_test3))\n",
    "\n",
    "    datas = {\n",
    "        'train': _gen_data(dec_train, horizon),\n",
    "        'val': _gen_data(dec_val, horizon),\n",
    "        'test': _gen_data(dec_test, horizon),\n",
    "    }\n",
    "    return datas\n",
    "\n",
    "\n",
    "def load_saved_datas(max_number=None):\n",
    "    \"\"\"\n",
    "    kinds = 'test', 'train', 'val'\n",
    "    \"\"\"\n",
    "    datas = {}\n",
    "    for kind in ['train', 'val', 'test']:\n",
    "        try:\n",
    "            with open(f'saved_data/x_{kind}.npy', 'rb') as file:\n",
    "                x = np.load(file)\n",
    "            with open(f'saved_data/y_{kind}.npy', 'rb') as file:\n",
    "                y = np.load(file)\n",
    "            if max_number is not None:\n",
    "                x = x[:max_number]\n",
    "                y = y[:max_number]\n",
    "        except FileNotFoundError:\n",
    "            x, y = None, None\n",
    "\n",
    "        datas.update({kind: [x, y]})\n",
    "    return datas\n",
    "\n",
    "\n",
    "def inspect_data(data, name='data'):\n",
    "    if data is not None:\n",
    "        x = data[0]\n",
    "        y = data[1]\n",
    "        print(f'{name: <10}: x= {str(x.shape): <15} | y= {str(y.shape): <15}')\n",
    "    else:\n",
    "        print(f'{name <10}: None')\n",
    "\n",
    "\n",
    "def inspect_datas(datas: dict):\n",
    "    print('    Datas:')\n",
    "    for name in datas:\n",
    "        data = datas[name]\n",
    "        inspect_data(data, name)\n",
    "\n",
    "\n",
    "# build datasets\n",
    "def build_dataset(\n",
    "    x: np.ndarray,\n",
    "    y: np.ndarray,\n",
    "    seq_len,\n",
    "    batch_size=128,\n",
    "    **timeseries_kwargs,\n",
    "):\n",
    "\n",
    "    def set_shape(value_x, value_y):\n",
    "        value_x.set_shape((None, seq_len, x.shape[-1]))\n",
    "        return value_x, value_y\n",
    "\n",
    "    ds = _timeseries_dataset_from_array(\n",
    "        data=x,\n",
    "        targets=y,\n",
    "        batch_size=batch_size,\n",
    "        sequence_length=seq_len,\n",
    "        **timeseries_kwargs,\n",
    "    )\n",
    "\n",
    "    return ds.map(set_shape)\n",
    "\n",
    "\n",
    "def build_datasets(datas: dict, batch_size, seq_len):\n",
    "    datasets = {}\n",
    "    for kind in datas:\n",
    "        data = datas.get(kind, None)\n",
    "        ds = None\n",
    "        if data is not None:\n",
    "            ds = build_dataset(\n",
    "                x=data[0],\n",
    "                y=data[1],\n",
    "                batch_size=batch_size,\n",
    "                seq_len=seq_len,\n",
    "            )\n",
    "        datasets.update({kind: ds})\n",
    "\n",
    "    return datasets\n",
    "\n",
    "\n",
    "def inspect_dataset(ds, name='dataset'):\n",
    "    if ds is not None:\n",
    "        print(f'{name: <10}: {[len(ds)]+ list(ds.element_spec[0].shape)[1:]}')\n",
    "    else:\n",
    "        print(f'{name <10}: None')\n",
    "\n",
    "\n",
    "def inspect_datasets(datasets: dict):\n",
    "    print('    Datasets:')\n",
    "    for name in datasets:\n",
    "        ds = datasets[name]\n",
    "        inspect_dataset(ds, name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataClass:\n",
    "    \"\"\"\n",
    "    make only lover case parametrs and not start with _\n",
    "    All this methods (exept __call__) only for beauty representation :)\n",
    "    \"\"\"\n",
    "\n",
    "    @staticmethod\n",
    "    def __not_data(field=None, get=False, not_data_fields: set = set()):\n",
    "        if not get:\n",
    "            not_data_fields.add(field.__name__)\n",
    "            return field\n",
    "        else:\n",
    "            return not_data_fields\n",
    "\n",
    "    def __new__(\n",
    "        cls,\n",
    "        target_dict: dict = None,\n",
    "        name: str = '',\n",
    "    ):\n",
    "        \"\"\"\n",
    "        build from nested dict\n",
    "        \"\"\"\n",
    "        if target_dict is not None:\n",
    "            result = DataClass()\n",
    "            return result.__rec_build(name, target_dict)\n",
    "        return super().__new__(cls)\n",
    "\n",
    "    def __rec_build(self, field_name: str, field):\n",
    "        if not isinstance(field, dict):\n",
    "            self.__setattr__(field_name, field)\n",
    "            return None\n",
    "\n",
    "        result = DataClass()\n",
    "        self.__setattr__(field_name, result)\n",
    "\n",
    "        for inner_field_name, inner_field in field.items():\n",
    "            inner_result = result.__rec_build(\n",
    "                inner_field_name,\n",
    "                inner_field,\n",
    "            )\n",
    "            if inner_result is not None:\n",
    "                self.__setattr__(field_name, inner_result)\n",
    "        return result\n",
    "\n",
    "    def __call__(self, **kwargs: dict):\n",
    "        \"\"\"\n",
    "        Set up parametrs\n",
    "        \"\"\"\n",
    "        for key, value in kwargs.items():\n",
    "            self.__setattr__(key, value)\n",
    "\n",
    "    def __get_all_fields(self):\n",
    "        # Add except fields\n",
    "\n",
    "        options = list(\n",
    "            filter(\n",
    "                lambda x:\n",
    "                (x[0] != '_') and (x not in self.__not_data(get=True)),\n",
    "                self.__dir__(),\n",
    "            ))\n",
    "        return options\n",
    "\n",
    "    def __repr__(self) -> str:\n",
    "        \"\"\"\n",
    "        Representation of options\n",
    "        \"\"\"\n",
    "        return self.__rec_print()[4:]\n",
    "\n",
    "    def _rec_print_depr(self, self_margin: str = ''):\n",
    "        if not isinstance(self, DataClass):\n",
    "            return f'{self}'\n",
    "\n",
    "        result = self_margin\n",
    "        for field_name in self.__get_all_fields():\n",
    "            inner_result = DataClass._rec_print_depr(\n",
    "                self.__getattribute__(field_name),\n",
    "                self_margin + ' ' * 4,\n",
    "            )\n",
    "            result += f'\\n{self_margin}{field_name}: {inner_result}'\n",
    "\n",
    "        if self_margin == '':\n",
    "            return result[1:]\n",
    "        else:\n",
    "            return result\n",
    "\n",
    "    def __rec_print(\n",
    "        self,\n",
    "        self_name: str = '',\n",
    "        self_header: str = '',\n",
    "        last=True,\n",
    "    ):\n",
    "        end = \"└─ \"\n",
    "        pipe = \"│  \"\n",
    "        tee = \"├─ \"\n",
    "        blank = \"   \"\n",
    "        result = f'{self_header}{end if last else tee}{self_name}\\n'\n",
    "\n",
    "        if not isinstance(self, DataClass):\n",
    "            if '<' in repr(self):\n",
    "                self = repr(self).split('at')[0].replace('<', '').strip()\n",
    "\n",
    "            return f'{self_header}{end if last else tee}{self_name}: {self}\\n'\n",
    "\n",
    "        fields = self.__get_all_fields()\n",
    "        for field_name in fields:\n",
    "            inner_result = DataClass.__rec_print(\n",
    "                self.__getattribute__(field_name),\n",
    "                self_name=field_name,\n",
    "                self_header=f'{self_header}{blank if last else pipe}',\n",
    "                last=field_name == fields[-1])\n",
    "\n",
    "            result += inner_result[6:]\n",
    "\n",
    "        return result\n",
    "\n",
    "    @property\n",
    "    @__not_data\n",
    "    def Info_nested(self):\n",
    "        \"\"\"\n",
    "        Containing options dict\n",
    "        \"\"\"\n",
    "        return self.__rec_nested()\n",
    "\n",
    "    def __rec_nested(self, self_name=None):\n",
    "        if not isinstance(self, DataClass):\n",
    "            return {self_name: self}\n",
    "\n",
    "        result = {}\n",
    "        for field_name in self.__get_all_fields():\n",
    "            inner_result = DataClass.__rec_nested(\n",
    "                self.__getattribute__(field_name),\n",
    "                field_name,\n",
    "            )\n",
    "            result.update(inner_result)\n",
    "\n",
    "        if self_name is None:\n",
    "            return result\n",
    "        else:\n",
    "            return {self_name: result}\n",
    "\n",
    "    @property\n",
    "    @__not_data\n",
    "    def Info_expanded(self):\n",
    "        return {\n",
    "            compound_key.strip()[2:]: value\n",
    "            for value, compound_key in self.__rec_expanded()\n",
    "        }\n",
    "\n",
    "    def __rec_expanded(self, composite_key=''):\n",
    "        if not isinstance(self, DataClass):\n",
    "            yield (self, composite_key)\n",
    "        else:\n",
    "            for field_name in self.__get_all_fields():\n",
    "                for inner_result in DataClass.__rec_expanded(\n",
    "                        self.__getattribute__(field_name),\n",
    "                        str(composite_key) + '__' + str(field_name),\n",
    "                ):\n",
    "                    yield inner_result\n",
    "\n",
    "    def __getitem__(self, value):\n",
    "        if isinstance(value, list | tuple):\n",
    "            result = {}\n",
    "            for i in value:\n",
    "                result.update({i: getattr(self, i, None)})\n",
    "            return DataClass(result)\n",
    "\n",
    "        result = getattr(self, value, None)\n",
    "        if isinstance(result, DataClass):\n",
    "            return DataClass(result.Info_nested)\n",
    "        else:\n",
    "            return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "BBHMPMJPC73f"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import keras\n",
    "import tensorflow as tf\n",
    "\n",
    "seq_len = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "oMaIqRf0C73g"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Datas:\n",
      "train     : x= (203799, 40)    | y= (203799,)      \n",
      "val       : x= (50949, 40)     | y= (50949,)       \n",
      "test      : x= (139586, 40)    | y= (139586,)      \n"
     ]
    }
   ],
   "source": [
    "# Load data\n",
    "row_data = (\n",
    "    # data.load_dataset(horizon=4)\n",
    "    load_saved_datas())\n",
    "\n",
    "inspect_datas(row_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "tkNDI_N-C73h"
   },
   "outputs": [],
   "source": [
    "# # Save data\n",
    "# data.save_data(name= 'train',x= x_train,y=y_train)\n",
    "# data.save_data(name= 'val',x= x_val,y=y_val)\n",
    "# data.save_data(name= 'test',x= x_test,y=y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "pApLC7P9C73j"
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Realisation transformer from article\n",
    "\"\"\"\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "\n",
    "from typing import Union as _Union\n",
    "from typing import Callable as _Callable\n",
    "from keras.utils import get_custom_objects as _get_custom_objects\n",
    "from keras import backend as _K\n",
    "\n",
    "\n",
    "# Input\n",
    "def input_block(seq_len):\n",
    "    inputs = keras.Input(shape=(seq_len, 40))\n",
    "    return inputs\n",
    "\n",
    "\n",
    "# CN\n",
    "def cnn_block(\n",
    "    input_layer,\n",
    "    filters,\n",
    "    dilation_steps,\n",
    "):\n",
    "    dilation_steps = [\n",
    "        2**dilation\n",
    "        for dilation in range(dilation_steps + 1)\n",
    "    ] # yapf: disable\n",
    "    x = input_layer\n",
    "    for dilation in dilation_steps:\n",
    "        layer = keras.layers.Conv1D(\n",
    "            filters=filters,\n",
    "            kernel_size=2,\n",
    "            dilation_rate=dilation,\n",
    "            activation='relu',\n",
    "            padding='causal',\n",
    "        )\n",
    "        x = layer(x)\n",
    "    return x\n",
    "\n",
    "\n",
    "# Normalisation\n",
    "def norm_block(input_layer):\n",
    "\n",
    "    norm = keras.layers.LayerNormalization()(input_layer)\n",
    "    return norm\n",
    "\n",
    "\n",
    "# Positional encoding\n",
    "class PositionalEncoding(keras.layers.Layer):\n",
    "\n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "\n",
    "    def call(self, x, *args, **kwargs):\n",
    "        steps, d_model = x.get_shape()[-2:]\n",
    "        ps = np.zeros([steps, 1], dtype=_K.floatx())\n",
    "        for step in range(steps):\n",
    "            ps[step, :] = [(2 / (steps - 1)) * step - 1]\n",
    "\n",
    "        ps_expand = _K.expand_dims(_K.constant(ps), axis=0)\n",
    "        ps_tiled = _K.tile(ps_expand, [_K.shape(x)[0], 1, 1])\n",
    "\n",
    "        x = _K.concatenate([x, ps_tiled], axis=-1)\n",
    "        return x\n",
    "\n",
    "\n",
    "def positional_encoder_block(input_layer):\n",
    "    pos = PositionalEncoding()(input_layer)\n",
    "    return pos\n",
    "\n",
    "\n",
    "# Transformer\n",
    "class MultiHeadSelfAttention(keras.layers.Layer):\n",
    "    \"\"\"\n",
    "    Base class for Multi-head Self-Attention layers.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, num_heads: int, use_masking: bool, **kwargs):\n",
    "        \"\"\"\n",
    "        :param num_heads: number of attention heads\n",
    "        :param use_masking: when True, forbids the attention to see the further\n",
    "          elements in the sequence.\n",
    "        :param kwargs: any extra arguments typical for a Keras layer,\n",
    "          such as name, etc.\n",
    "        \"\"\"\n",
    "        self.num_heads = num_heads\n",
    "        self.use_masking = use_masking\n",
    "        self.qkv_weights = None\n",
    "        super().__init__(**kwargs)\n",
    "\n",
    "    def get_config(self):\n",
    "        config = super().get_config()\n",
    "        config['num_heads'] = self.num_heads\n",
    "        config['use_masking'] = self.use_masking\n",
    "        return config\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        # if not isinstance(input_shape, TensorShape):\n",
    "        #     raise ValueError('Invalid input')\n",
    "        d_model = input_shape[-1]\n",
    "\n",
    "        self.validate_model_dimensionality(d_model)\n",
    "        self.qkv_weights = self.add_weight(\n",
    "            name='qkv_weights',\n",
    "            shape=(d_model, d_model * 3),  # * 3 for q, k and v\n",
    "            initializer='glorot_uniform',\n",
    "            trainable=True)\n",
    "\n",
    "        return super().build(input_shape)\n",
    "\n",
    "    def call(self, inputs, **kwargs):\n",
    "        # if not K.is_keras_tensor(inputs):\n",
    "        #     raise ValueError(\n",
    "        #         'The layer can be called only with one tensor as an argument')\n",
    "        _, seq_len, d_model = _K.int_shape(inputs)\n",
    "\n",
    "        # Perform affine transformations to get the Queries, the Keys and the Values.\n",
    "        qkv = _K.dot(inputs, self.qkv_weights)  # (-1,seq_len,d_model*3)\n",
    "        qkv = _K.reshape(qkv, [-1, d_model * 3])\n",
    "\n",
    "        # splitting the keys, the values and the queries.\n",
    "        pre_q, pre_k, pre_v = [\n",
    "            _K.reshape(\n",
    "                qkv[:, i * d_model:(i + 1) * d_model],\n",
    "                (-1, seq_len, self.num_heads, d_model // self.num_heads))\n",
    "            for i in range(3)\n",
    "        ]\n",
    "\n",
    "        attention_out = self.attention(\n",
    "            pre_q,\n",
    "            pre_v,\n",
    "            pre_k,\n",
    "            seq_len,\n",
    "            d_model,\n",
    "            training=kwargs.get('training'),\n",
    "        )\n",
    "        # of shape (-1, seq_len, d_model)\n",
    "        return attention_out\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        shape_a, seq_len, d_model = input_shape\n",
    "        return shape_a, seq_len, d_model\n",
    "\n",
    "    def validate_model_dimensionality(self, d_model: int):\n",
    "        if d_model % self.num_heads != 0:\n",
    "            raise ValueError(\n",
    "                f'The size of the last dimension of the input '\n",
    "                f'({d_model}) must be evenly divisible by the number'\n",
    "                f'of the attention heads {self.num_heads}')\n",
    "\n",
    "    def attention(\n",
    "        self,\n",
    "        pre_q,\n",
    "        pre_v,\n",
    "        pre_k,\n",
    "        seq_len: int,\n",
    "        d_model: int,\n",
    "        training=None,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Calculates the output of the attention once the affine transformations\n",
    "        of the inputs are done. Here's the shapes of the arguments:\n",
    "        :param pre_q: (batch_size, q_seq_len, num_heads, d_model // num_heads)\n",
    "        :param pre_v: (batch_size, v_seq_len, num_heads, d_model // num_heads)\n",
    "        :param pre_k: (batch_size, k_seq_len, num_heads, d_model // num_heads)\n",
    "        :param seq_len: the length of the output sequence\n",
    "        :param d_model: dimensionality of the model (by the paper)\n",
    "        :param training: Passed by Keras. Should not be defined manually.\n",
    "          Optional scalar tensor indicating if we're in training\n",
    "          or inference phase.\n",
    "        \"\"\"\n",
    "        d_submodel = d_model // self.num_heads\n",
    "\n",
    "        # shaping Q and V into (batch_size, num_heads, seq_len, d_model//heads)\n",
    "        q = _K.permute_dimensions(pre_q, [0, 2, 1, 3])\n",
    "        v = _K.permute_dimensions(pre_v, [0, 2, 1, 3])\n",
    "        k = _K.permute_dimensions(pre_k, [0, 2, 3, 1])\n",
    "\n",
    "        q = _K.reshape(q, (-1, seq_len, d_submodel))\n",
    "        k = _K.reshape(k, (-1, seq_len, d_submodel))\n",
    "        v = _K.reshape(v, (-1, seq_len, d_submodel))\n",
    "        qk = tf.einsum('aib,ajb->aij', q, k)\n",
    "        sqrt_d = _K.constant(np.sqrt(d_model // self.num_heads),\n",
    "                             dtype=_K.floatx())\n",
    "        a = qk / sqrt_d\n",
    "        a = self.mask_attention(a)\n",
    "        a = _K.softmax(a)\n",
    "        attention_heads = tf.einsum('aij,ajb->aib', a, v)\n",
    "        attention_heads = _K.reshape(attention_heads,\n",
    "                                     (-1, self.num_heads, seq_len, d_submodel))\n",
    "        attention_heads = _K.permute_dimensions(attention_heads, [0, 2, 1, 3])\n",
    "        attention_heads = _K.reshape(attention_heads, (-1, seq_len, d_model))\n",
    "\n",
    "        return attention_heads\n",
    "\n",
    "    def mask_attention(self, dot_product):\n",
    "        \"\"\"\n",
    "        Makes sure that (when enabled) each position\n",
    "        (of a decoder's self-attention) cannot attend to subsequent positions.\n",
    "        :param dot_product: scaled dot-product of Q and K after reshaping them\n",
    "        to 3D tensors (batch * num_heads, rows, cols)\n",
    "        \"\"\"\n",
    "        if not self.use_masking:\n",
    "            return dot_product\n",
    "        last_dims = _K.int_shape(dot_product)[-2:]\n",
    "        low_triangle_ones = (\n",
    "            np.tril(np.ones(last_dims))\n",
    "            # to ensure proper broadcasting\n",
    "            .reshape((1, ) + last_dims))\n",
    "        inverse_low_triangle = 1 - low_triangle_ones\n",
    "        close_to_negative_inf = -1e9\n",
    "        result = (\n",
    "            _K.constant(low_triangle_ones, dtype=_K.floatx()) * dot_product +\n",
    "            _K.constant(close_to_negative_inf * inverse_low_triangle))\n",
    "        return result\n",
    "\n",
    "\n",
    "_get_custom_objects().update({\n",
    "    'MultiHeadSelfAttention': MultiHeadSelfAttention,\n",
    "})\n",
    "\n",
    "\n",
    "class CustomNormalization(keras.layers.Layer):\n",
    "    \"\"\"\n",
    "    Implementation of Layer Normalization (https://arxiv.org/abs/1607.06450).\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, axis=-1, **kwargs):\n",
    "        self.axis = axis\n",
    "        super().__init__(**kwargs)\n",
    "\n",
    "    def get_config(self):\n",
    "        config = super().get_config()\n",
    "        config['axis'] = self.axis\n",
    "        return config\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        dim = input_shape[-1]\n",
    "        self.gain = self.add_weight(\n",
    "            name='gain',\n",
    "            shape=(dim, ),\n",
    "            initializer='ones',\n",
    "            trainable=True,\n",
    "        )\n",
    "        self.bias = self.add_weight(\n",
    "            name='bias',\n",
    "            shape=(dim, ),\n",
    "            initializer='zeros',\n",
    "            trainable=True,\n",
    "        )\n",
    "        return super().build(input_shape)\n",
    "\n",
    "    def call(self, inputs, **kwargs):\n",
    "        mean = _K.mean(\n",
    "            inputs,\n",
    "            axis=self.axis,\n",
    "            keepdims=True,\n",
    "        )\n",
    "        variance = _K.mean(\n",
    "            _K.square(inputs - mean),\n",
    "            axis=self.axis,\n",
    "            keepdims=True,\n",
    "        )\n",
    "        epsilon = _K.constant(\n",
    "            1e-5,\n",
    "            dtype=_K.floatx(),\n",
    "        )\n",
    "        normalized_inputs = (inputs - mean) / _K.sqrt(variance + epsilon)\n",
    "        result = self.gain * normalized_inputs + self.bias\n",
    "        return result\n",
    "\n",
    "\n",
    "class TransformerTransition(keras.layers.Layer):\n",
    "    \"\"\"\n",
    "    Transformer transition function. The same function is used both\n",
    "    in classical in Universal Transformers.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        activation: _Union[str, _Callable],\n",
    "        size_multiplier: int = 4,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        :param activation: activation function. Must be a string or a callable.\n",
    "        :param size_multiplier: How big the hidden dimension should be.\n",
    "          Most of the implementation use transition functions having 4 times\n",
    "          more hidden units than the model itself.\n",
    "        :param kwargs: Keras-specific layer arguments.\n",
    "        \"\"\"\n",
    "        self.activation = keras.activations.get(activation)\n",
    "        self.size_multiplier = size_multiplier\n",
    "        super().__init__(**kwargs)\n",
    "\n",
    "    def get_config(self):\n",
    "        config = super().get_config()\n",
    "        config['activation'] = keras.activations.serialize(self.activation)\n",
    "        config['size_multiplier'] = self.size_multiplier\n",
    "        return config\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        d_model = input_shape[-1]\n",
    "        self.weights1 = self.add_weight(\n",
    "            name='weights1',\n",
    "            shape=(d_model, self.size_multiplier * d_model),\n",
    "            initializer='glorot_uniform',\n",
    "            trainable=True,\n",
    "        )\n",
    "        self.biases1 = self.add_weight(\n",
    "            name='biases1',\n",
    "            shape=(self.size_multiplier * d_model),\n",
    "            initializer='zeros',\n",
    "            trainable=True,\n",
    "        )\n",
    "        self.weights2 = self.add_weight(\n",
    "            name='weights2',\n",
    "            shape=(self.size_multiplier * d_model, d_model),\n",
    "            initializer='glorot_uniform',\n",
    "            trainable=True,\n",
    "        )\n",
    "        self.biases2 = self.add_weight(\n",
    "            name='biases2',\n",
    "            shape=(d_model, ),\n",
    "            initializer='zeros',\n",
    "            trainable=True,\n",
    "        )\n",
    "        return super().build(input_shape)\n",
    "\n",
    "    def call(self, inputs, **kwargs):\n",
    "        input_shape = _K.int_shape(inputs)\n",
    "        d_model = input_shape[-1]\n",
    "\n",
    "        K_dot = _K.dot(_K.reshape(inputs, (-1, d_model)), self.weights1)\n",
    "        step1 = self.activation(\n",
    "            _K.bias_add(K_dot, self.biases1, data_format='channels_last'))\n",
    "\n",
    "        K_dot = _K.dot(step1, self.weights2)\n",
    "        step2 = _K.bias_add(K_dot, self.biases2, data_format='channels_last')\n",
    "        result = _K.reshape(step2, (-1, ) + input_shape[-2:])\n",
    "        return result\n",
    "\n",
    "\n",
    "class TransformerLayer(keras.layers.Layer):\n",
    "    \"\"\"\n",
    "    A pseudo-layer combining together all nuts and bolts to assemble\n",
    "    a complete section of both the Transformer and the Universal Transformer\n",
    "    models, following description from the \"Universal Transformers\" paper.\n",
    "    Each such block is, essentially:\n",
    "    - Multi-head self-attention (masked or unmasked)\n",
    "    - Residual connection,\n",
    "    - Layer normalization\n",
    "    - Transition function\n",
    "    - Residual connection\n",
    "    - Layer normalization\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        # name: str,\n",
    "        num_heads: int,\n",
    "        use_masking: bool = True,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        self.attention_layer = MultiHeadSelfAttention(\n",
    "            num_heads,\n",
    "            use_masking=use_masking,\n",
    "            # name=f'{name}_self_attention',\n",
    "        )\n",
    "        self.norm1_layer = CustomNormalization()\n",
    "        self.norm2_layer = CustomNormalization()\n",
    "        self.transition_layer = TransformerTransition(activation='relu', )\n",
    "        self.addition_layer = keras.layers.Add()\n",
    "        super().__init__(**kwargs)\n",
    "\n",
    "    def call(self, x, **kwargs):\n",
    "        #PostLN: X -> attention -> +X -> norm1 -> transition -> +norm1 -> norm2\n",
    "        attention = self.attention_layer(x)\n",
    "        residual_1 = (self.addition_layer([x, attention]))\n",
    "        norm_1 = self.norm1_layer(residual_1)\n",
    "\n",
    "        transition = self.transition_layer(norm_1)\n",
    "        residual_2 = (self.addition_layer([norm_1, transition]))\n",
    "        norm_2 = self.norm2_layer(residual_2)\n",
    "\n",
    "        return norm_2\n",
    "\n",
    "\n",
    "def transformer_block(\n",
    "    input_layer,\n",
    "    share_weights,\n",
    "    n_blocks,\n",
    "    n_heads,\n",
    "):\n",
    "    x = input_layer\n",
    "    tb = TransformerLayer(\n",
    "        num_heads=n_heads,\n",
    "        use_masking=True,\n",
    "    )\n",
    "    for block in range(n_blocks):\n",
    "        if share_weights:\n",
    "            x = tb(x)\n",
    "        else:\n",
    "            x = TransformerLayer(\n",
    "                num_heads=n_heads,\n",
    "                use_masking=True,\n",
    "            )(x)\n",
    "\n",
    "    return x\n",
    "\n",
    "\n",
    "# FFN\n",
    "def ffn_block(\n",
    "    input_layer,\n",
    "    dropout_rate,\n",
    "    activation,\n",
    "    units,\n",
    "    kernel_regularizer,\n",
    "    kernel_initializer,\n",
    "):\n",
    "    input_layer = keras.layers.Flatten()(input_layer)\n",
    "\n",
    "    input_layer = keras.layers.Dense(\n",
    "        units=units,\n",
    "        activation=activation,\n",
    "        kernel_regularizer=kernel_regularizer,\n",
    "        kernel_initializer=kernel_initializer,\n",
    "    )(input_layer)\n",
    "\n",
    "    input_layer = keras.layers.Dropout(dropout_rate)(input_layer)\n",
    "    out = keras.layers.Dense(\n",
    "        units=3,\n",
    "        activation='softmax',\n",
    "    )(input_layer)\n",
    "    return out\n",
    "\n",
    "\n",
    "# Collection\n",
    "class blocks:\n",
    "    input_block = input_block\n",
    "    cnn_block = cnn_block\n",
    "    norm_block = norm_block\n",
    "    positional_encoder_block = positional_encoder_block\n",
    "    transformer_block = transformer_block\n",
    "    ffn_block = ffn_block\n",
    "\n",
    "\n",
    "# parametrs\n",
    "PARAMETRS = {\n",
    "    'seq_len': 100,\n",
    "    'cn': dict(\n",
    "        n_filters=14,\n",
    "        dilation_steps=4,\n",
    "    ),\n",
    "    'an': dict(\n",
    "        attention_heads=3,\n",
    "        blocks=2,\n",
    "        share_weights=False,\n",
    "    ),\n",
    "    'ff': dict(\n",
    "        units = 64,\n",
    "        dropout_rate=0.1,\n",
    "        activation=keras.activations.relu,\n",
    "        kernel_regularizer=keras.regularizers.L2(),\n",
    "        kernel_initializer='glorot_uniform',\n",
    "    ),\n",
    "    'optimizer':\n",
    "    keras.optimizers.legacy.Adam(\n",
    "        learning_rate=0.0001,\n",
    "        beta_1=0.9,\n",
    "        beta_2=0.999,\n",
    "    ),\n",
    "} #yapf:disable\n",
    "\n",
    "\n",
    "# build\n",
    "def build_model(\n",
    "    seq_len,\n",
    "    cn__n_filters,\n",
    "    cn__dilation_steps,\n",
    "    an__blocks,\n",
    "    an__attention_heads,\n",
    "    an__share_weights,\n",
    "    ff__units,\n",
    "    ff__dropout_rate,\n",
    "    ff__activation,\n",
    "    ff__kernel_regularizer,\n",
    "    ff__kernel_initializer,\n",
    "    optimizer,\n",
    "):\n",
    "    # Model\n",
    "    inputs = blocks.input_block(seq_len)\n",
    "    x = inputs\n",
    "    x = blocks.cnn_block(\n",
    "        input_layer=x,\n",
    "        filters=cn__n_filters,\n",
    "        dilation_steps=cn__dilation_steps,\n",
    "    )\n",
    "    x = blocks.norm_block(input_layer=x)\n",
    "    x = blocks.positional_encoder_block(input_layer=x)\n",
    "    x = blocks.transformer_block(\n",
    "        input_layer=x,\n",
    "        n_blocks=an__blocks,\n",
    "        n_heads=an__attention_heads,\n",
    "        share_weights=an__share_weights,\n",
    "    )\n",
    "    x = blocks.ffn_block(\n",
    "        input_layer=x,\n",
    "        units=ff__units,\n",
    "        dropout_rate=ff__dropout_rate,\n",
    "        activation=ff__activation,\n",
    "        kernel_regularizer=ff__kernel_regularizer,\n",
    "        kernel_initializer=ff__kernel_initializer,\n",
    "    )\n",
    "\n",
    "    model = keras.Model(inputs=inputs, outputs=x)\n",
    "\n",
    "    # Compile\n",
    "    model.compile(\n",
    "        optimizer,\n",
    "        loss=keras.losses.SparseCategoricalCrossentropy(),\n",
    "        metrics=[\n",
    "            keras.metrics.SparseCategoricalAccuracy(name='sp_acc'),\n",
    "            keras.metrics.CategoricalAccuracy(name='acc'),\n",
    "        ],\n",
    "    )\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "5W3fzwPnC73k"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Datasets:\n",
      "train     : [398, 100, 40]\n",
      "val       : [100, 100, 40]\n",
      "test      : [273, 100, 40]\n"
     ]
    }
   ],
   "source": [
    "# Datasets\n",
    "datasets = build_datasets(\n",
    "    datas=row_data,\n",
    "    batch_size=512,\n",
    "    seq_len=seq_len,\n",
    ")\n",
    "(ds_train, ds_val, ds_test) =\\\n",
    "(datasets['train'], datasets['val'], datasets['test'])\n",
    "inspect_datasets(datasets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "5sSnl4VYC73k"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_1 (InputLayer)        [(None, 100, 40)]         0         \n",
      "                                                                 \n",
      " conv1d (Conv1D)             (None, 100, 14)           1134      \n",
      "                                                                 \n",
      " conv1d_1 (Conv1D)           (None, 100, 14)           406       \n",
      "                                                                 \n",
      " conv1d_2 (Conv1D)           (None, 100, 14)           406       \n",
      "                                                                 \n",
      " conv1d_3 (Conv1D)           (None, 100, 14)           406       \n",
      "                                                                 \n",
      " conv1d_4 (Conv1D)           (None, 100, 14)           406       \n",
      "                                                                 \n",
      " layer_normalization (Layer  (None, 100, 14)           28        \n",
      " Normalization)                                                  \n",
      "                                                                 \n",
      " positional_encoding (Posit  (None, 100, 15)           0         \n",
      " ionalEncoding)                                                  \n",
      "                                                                 \n",
      " transformer_layer_1 (Trans  (None, 100, 15)           2610      \n",
      " formerLayer)                                                    \n",
      "                                                                 \n",
      " transformer_layer_2 (Trans  (None, 100, 15)           2610      \n",
      " formerLayer)                                                    \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 1500)              0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 64)                96064     \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 64)                0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 3)                 195       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 104265 (407.29 KB)\n",
      "Trainable params: 104265 (407.29 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Build\n",
    "model_name = 'base_model_low'\n",
    "pars = DataClass(PARAMETRS)\n",
    "model = build_model(**pars.Info_expanded)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "2opagWnEC73l"
   },
   "outputs": [],
   "source": [
    "# Callbacks\n",
    "callbacks = [\n",
    "    keras.callbacks.TensorBoard(\n",
    "        f\"Temp/callbacks/{model_name}\",\n",
    "        histogram_freq=1,\n",
    "        update_freq=1,\n",
    "    ),\n",
    "    tf.keras.callbacks.ModelCheckpoint(\n",
    "        f\"Temp/callbacks/{model_name}/checkPoint\",\n",
    "        monitor=\"val_loss\",\n",
    "        verbose=0,\n",
    "        save_best_only=False,\n",
    "        save_weights_only=True,\n",
    "        mode=\"auto\",\n",
    "        save_freq=50,\n",
    "        options=None,\n",
    "        initial_value_threshold=None,\n",
    "    )\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "IbACOcK8jpUg"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "  3/398 [..............................] - ETA: 7:38 - loss: 4.0748 - sp_acc: 0.2337 - acc: 0.0000e+00"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39m# Train\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m model\u001b[39m.\u001b[39;49mfit(\n\u001b[0;32m      3\u001b[0m     ds_train,\n\u001b[0;32m      4\u001b[0m     epochs\u001b[39m=\u001b[39;49m\u001b[39m20\u001b[39;49m,\n\u001b[0;32m      5\u001b[0m     validation_data\u001b[39m=\u001b[39;49mds_val,\n\u001b[0;32m      6\u001b[0m     callbacks\u001b[39m=\u001b[39;49mcallbacks,\n\u001b[0;32m      7\u001b[0m )\n",
      "File \u001b[1;32md:\\WORKS\\translob\\translob_env\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     63\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m     64\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m---> 65\u001b[0m     \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m     66\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32md:\\WORKS\\translob\\translob_env\\Lib\\site-packages\\keras\\src\\engine\\training.py:1742\u001b[0m, in \u001b[0;36mModel.fit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1734\u001b[0m \u001b[39mwith\u001b[39;00m tf\u001b[39m.\u001b[39mprofiler\u001b[39m.\u001b[39mexperimental\u001b[39m.\u001b[39mTrace(\n\u001b[0;32m   1735\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mtrain\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m   1736\u001b[0m     epoch_num\u001b[39m=\u001b[39mepoch,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1739\u001b[0m     _r\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m,\n\u001b[0;32m   1740\u001b[0m ):\n\u001b[0;32m   1741\u001b[0m     callbacks\u001b[39m.\u001b[39mon_train_batch_begin(step)\n\u001b[1;32m-> 1742\u001b[0m     tmp_logs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtrain_function(iterator)\n\u001b[0;32m   1743\u001b[0m     \u001b[39mif\u001b[39;00m data_handler\u001b[39m.\u001b[39mshould_sync:\n\u001b[0;32m   1744\u001b[0m         context\u001b[39m.\u001b[39masync_wait()\n",
      "File \u001b[1;32md:\\WORKS\\translob\\translob_env\\Lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    148\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m    149\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 150\u001b[0m   \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m    151\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m    152\u001b[0m   filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32md:\\WORKS\\translob\\translob_env\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\polymorphic_function.py:825\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    822\u001b[0m compiler \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mxla\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jit_compile \u001b[39melse\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mnonXla\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    824\u001b[0m \u001b[39mwith\u001b[39;00m OptionalXlaContext(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jit_compile):\n\u001b[1;32m--> 825\u001b[0m   result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwds)\n\u001b[0;32m    827\u001b[0m new_tracing_count \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mexperimental_get_tracing_count()\n\u001b[0;32m    828\u001b[0m without_tracing \u001b[39m=\u001b[39m (tracing_count \u001b[39m==\u001b[39m new_tracing_count)\n",
      "File \u001b[1;32md:\\WORKS\\translob\\translob_env\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\polymorphic_function.py:857\u001b[0m, in \u001b[0;36mFunction._call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    854\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock\u001b[39m.\u001b[39mrelease()\n\u001b[0;32m    855\u001b[0m   \u001b[39m# In this case we have created variables on the first call, so we run the\u001b[39;00m\n\u001b[0;32m    856\u001b[0m   \u001b[39m# defunned version which is guaranteed to never create variables.\u001b[39;00m\n\u001b[1;32m--> 857\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_no_variable_creation_fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwds)  \u001b[39m# pylint: disable=not-callable\u001b[39;00m\n\u001b[0;32m    858\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_variable_creation_fn \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    859\u001b[0m   \u001b[39m# Release the lock early so that multiple threads can perform the call\u001b[39;00m\n\u001b[0;32m    860\u001b[0m   \u001b[39m# in parallel.\u001b[39;00m\n\u001b[0;32m    861\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock\u001b[39m.\u001b[39mrelease()\n",
      "File \u001b[1;32md:\\WORKS\\translob\\translob_env\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\tracing_compiler.py:148\u001b[0m, in \u001b[0;36mTracingCompiler.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    145\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock:\n\u001b[0;32m    146\u001b[0m   (concrete_function,\n\u001b[0;32m    147\u001b[0m    filtered_flat_args) \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_maybe_define_function(args, kwargs)\n\u001b[1;32m--> 148\u001b[0m \u001b[39mreturn\u001b[39;00m concrete_function\u001b[39m.\u001b[39;49m_call_flat(\n\u001b[0;32m    149\u001b[0m     filtered_flat_args, captured_inputs\u001b[39m=\u001b[39;49mconcrete_function\u001b[39m.\u001b[39;49mcaptured_inputs)\n",
      "File \u001b[1;32md:\\WORKS\\translob\\translob_env\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\monomorphic_function.py:1349\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[1;34m(self, args, captured_inputs)\u001b[0m\n\u001b[0;32m   1345\u001b[0m possible_gradient_type \u001b[39m=\u001b[39m gradients_util\u001b[39m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[0;32m   1346\u001b[0m \u001b[39mif\u001b[39;00m (possible_gradient_type \u001b[39m==\u001b[39m gradients_util\u001b[39m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[0;32m   1347\u001b[0m     \u001b[39mand\u001b[39;00m executing_eagerly):\n\u001b[0;32m   1348\u001b[0m   \u001b[39m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[1;32m-> 1349\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_build_call_outputs(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_inference_function(\u001b[39m*\u001b[39;49margs))\n\u001b[0;32m   1350\u001b[0m forward_backward \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[0;32m   1351\u001b[0m     args,\n\u001b[0;32m   1352\u001b[0m     possible_gradient_type,\n\u001b[0;32m   1353\u001b[0m     executing_eagerly)\n\u001b[0;32m   1354\u001b[0m forward_function, args_with_tangents \u001b[39m=\u001b[39m forward_backward\u001b[39m.\u001b[39mforward()\n",
      "File \u001b[1;32md:\\WORKS\\translob\\translob_env\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\atomic_function.py:196\u001b[0m, in \u001b[0;36mAtomicFunction.__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m    194\u001b[0m \u001b[39mwith\u001b[39;00m record\u001b[39m.\u001b[39mstop_recording():\n\u001b[0;32m    195\u001b[0m   \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_bound_context\u001b[39m.\u001b[39mexecuting_eagerly():\n\u001b[1;32m--> 196\u001b[0m     outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_bound_context\u001b[39m.\u001b[39;49mcall_function(\n\u001b[0;32m    197\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mname,\n\u001b[0;32m    198\u001b[0m         \u001b[39mlist\u001b[39;49m(args),\n\u001b[0;32m    199\u001b[0m         \u001b[39mlen\u001b[39;49m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfunction_type\u001b[39m.\u001b[39;49mflat_outputs),\n\u001b[0;32m    200\u001b[0m     )\n\u001b[0;32m    201\u001b[0m   \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    202\u001b[0m     outputs \u001b[39m=\u001b[39m make_call_op_in_graph(\u001b[39mself\u001b[39m, \u001b[39mlist\u001b[39m(args))\n",
      "File \u001b[1;32md:\\WORKS\\translob\\translob_env\\Lib\\site-packages\\tensorflow\\python\\eager\\context.py:1457\u001b[0m, in \u001b[0;36mContext.call_function\u001b[1;34m(self, name, tensor_inputs, num_outputs)\u001b[0m\n\u001b[0;32m   1455\u001b[0m cancellation_context \u001b[39m=\u001b[39m cancellation\u001b[39m.\u001b[39mcontext()\n\u001b[0;32m   1456\u001b[0m \u001b[39mif\u001b[39;00m cancellation_context \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m-> 1457\u001b[0m   outputs \u001b[39m=\u001b[39m execute\u001b[39m.\u001b[39;49mexecute(\n\u001b[0;32m   1458\u001b[0m       name\u001b[39m.\u001b[39;49mdecode(\u001b[39m\"\u001b[39;49m\u001b[39mutf-8\u001b[39;49m\u001b[39m\"\u001b[39;49m),\n\u001b[0;32m   1459\u001b[0m       num_outputs\u001b[39m=\u001b[39;49mnum_outputs,\n\u001b[0;32m   1460\u001b[0m       inputs\u001b[39m=\u001b[39;49mtensor_inputs,\n\u001b[0;32m   1461\u001b[0m       attrs\u001b[39m=\u001b[39;49mattrs,\n\u001b[0;32m   1462\u001b[0m       ctx\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m,\n\u001b[0;32m   1463\u001b[0m   )\n\u001b[0;32m   1464\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m   1465\u001b[0m   outputs \u001b[39m=\u001b[39m execute\u001b[39m.\u001b[39mexecute_with_cancellation(\n\u001b[0;32m   1466\u001b[0m       name\u001b[39m.\u001b[39mdecode(\u001b[39m\"\u001b[39m\u001b[39mutf-8\u001b[39m\u001b[39m\"\u001b[39m),\n\u001b[0;32m   1467\u001b[0m       num_outputs\u001b[39m=\u001b[39mnum_outputs,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1471\u001b[0m       cancellation_manager\u001b[39m=\u001b[39mcancellation_context,\n\u001b[0;32m   1472\u001b[0m   )\n",
      "File \u001b[1;32md:\\WORKS\\translob\\translob_env\\Lib\\site-packages\\tensorflow\\python\\eager\\execute.py:53\u001b[0m, in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     51\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m     52\u001b[0m   ctx\u001b[39m.\u001b[39mensure_initialized()\n\u001b[1;32m---> 53\u001b[0m   tensors \u001b[39m=\u001b[39m pywrap_tfe\u001b[39m.\u001b[39;49mTFE_Py_Execute(ctx\u001b[39m.\u001b[39;49m_handle, device_name, op_name,\n\u001b[0;32m     54\u001b[0m                                       inputs, attrs, num_outputs)\n\u001b[0;32m     55\u001b[0m \u001b[39mexcept\u001b[39;00m core\u001b[39m.\u001b[39m_NotOkStatusException \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m     56\u001b[0m   \u001b[39mif\u001b[39;00m name \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Train\n",
    "model.fit(\n",
    "    ds_train,\n",
    "    epochs=20,\n",
    "    validation_data=ds_val,\n",
    "    callbacks=callbacks,\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11.1 ('translob_env': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "9a9191c6b2d33302590e376f8aee71d3c7e87a446da0bda5474eb7c47655c891"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
